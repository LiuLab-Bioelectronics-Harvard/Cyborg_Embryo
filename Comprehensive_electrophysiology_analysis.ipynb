{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db322f60-1286-43e2-b65c-26bfb5e2b815",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import necessary packages\"\"\"\n",
    "\n",
    "import sys, struct, math, os, time\n",
    "import numpy as np\n",
    "from ismember import ismember\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import zscore, ttest_rel, pearsonr\n",
    "from scipy import signal, special\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + '/util/')\n",
    "sys.path.append('./util/')\n",
    "from intanutil.read_header import read_header\n",
    "from intanutil.get_bytes_per_data_block import get_bytes_per_data_block\n",
    "from intanutil.read_one_data_block import read_one_data_block\n",
    "from intanutil.notch_filter import notch_filter\n",
    "from intanutil.data_to_result import data_to_result\n",
    "\n",
    "import spikeinterface\n",
    "spikeinterface.__version__\n",
    "import spikeinterface\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.toolkit as st\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import matplotlib.pyplot as plt\n",
    "from spikeinterface import WaveformExtractor\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from spikeinterface.core.npzsortingextractor import NpzSortingExtractor\n",
    "from pylab import *\n",
    "ss.Kilosort3Sorter.set_kilosort3_path('/kilosort3')\n",
    "import pylab\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pprint import pprint\n",
    "\n",
    "from probeinterface.utils import combine_probes\n",
    "from probeinterface import generate_multi_columns_probe, Probe\n",
    "from probeinterface.plotting import plot_probe\n",
    "\n",
    "import copy\n",
    "from util.brpylib import NsxFile, brpylib_ver\n",
    "from calculate_features import features_5\n",
    "\n",
    "from CurationTool import shank_autocorrelogram_show, shank_correlogram_show\n",
    "from spikeinterface.sorters import WaveClusSorter, IronClustSorter, Kilosort3Sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601bdde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Reads Intan Technologies RHD2000 data file generated by evaluation board GUI.\n",
    "    \n",
    "    Data are returned in a dictionary, for future extensibility.\n",
    "    \"\"\"\n",
    "\n",
    "    tic = time.time()\n",
    "    fid = open(filename, 'rb')\n",
    "    filesize = os.path.getsize(filename)\n",
    "\n",
    "    header = read_header(fid)\n",
    "\n",
    "    print('Found {} amplifier channel{}.'.format(header['num_amplifier_channels'], plural(header['num_amplifier_channels'])))\n",
    "    print('Found {} auxiliary input channel{}.'.format(header['num_aux_input_channels'], plural(header['num_aux_input_channels'])))\n",
    "    print('Found {} supply voltage channel{}.'.format(header['num_supply_voltage_channels'], plural(header['num_supply_voltage_channels'])))\n",
    "    print('Found {} board ADC channel{}.'.format(header['num_board_adc_channels'], plural(header['num_board_adc_channels'])))\n",
    "    print('Found {} board digital input channel{}.'.format(header['num_board_dig_in_channels'], plural(header['num_board_dig_in_channels'])))\n",
    "    print('Found {} board digital output channel{}.'.format(header['num_board_dig_out_channels'], plural(header['num_board_dig_out_channels'])))\n",
    "    print('Found {} temperature sensors channel{}.'.format(header['num_temp_sensor_channels'], plural(header['num_temp_sensor_channels'])))\n",
    "    print('')\n",
    "\n",
    "    # Determine how many samples the data file contains.\n",
    "    bytes_per_block = get_bytes_per_data_block(header)\n",
    "\n",
    "    # How many data blocks remain in this file?\n",
    "    data_present = False\n",
    "    bytes_remaining = filesize - fid.tell()\n",
    "    if bytes_remaining > 0:\n",
    "        data_present = True\n",
    "\n",
    "    if bytes_remaining % bytes_per_block != 0:\n",
    "        raise Exception('Something is wrong with file size : should have a whole number of data blocks')\n",
    "\n",
    "    num_data_blocks = int(bytes_remaining / bytes_per_block)\n",
    "\n",
    "    num_amplifier_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "    num_aux_input_samples = int((header['num_samples_per_data_block'] / 4) * num_data_blocks)\n",
    "    num_supply_voltage_samples = 1 * num_data_blocks\n",
    "    num_board_adc_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "    num_board_dig_in_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "    num_board_dig_out_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "\n",
    "    record_time = num_amplifier_samples / header['sample_rate']\n",
    "\n",
    "    if data_present:\n",
    "        print('File contains {:0.3f} seconds of data.  Amplifiers were sampled at {:0.2f} kS/s.'.format(record_time, header['sample_rate'] / 1000))\n",
    "    else:\n",
    "        print('Header file contains no data.  Amplifiers were sampled at {:0.2f} kS/s.'.format(header['sample_rate'] / 1000))\n",
    "\n",
    "    if data_present:\n",
    "        # Pre-allocate memory for data.\n",
    "        print('')\n",
    "        print('Allocating memory for data...')\n",
    "\n",
    "        data = {}\n",
    "        if (header['version']['major'] == 1 and header['version']['minor'] >= 2) or (header['version']['major'] > 1):\n",
    "            data['t_amplifier'] = np.zeros(num_amplifier_samples, dtype=np.int)\n",
    "        else:\n",
    "            data['t_amplifier'] = np.zeros(num_amplifier_samples, dtype=np.uint)\n",
    "\n",
    "        data['amplifier_data'] = np.zeros([header['num_amplifier_channels'], num_amplifier_samples], dtype=np.uint)\n",
    "        data['aux_input_data'] = np.zeros([header['num_aux_input_channels'], num_aux_input_samples], dtype=np.uint)\n",
    "        data['supply_voltage_data'] = np.zeros([header['num_supply_voltage_channels'], num_supply_voltage_samples], dtype=np.uint)\n",
    "        data['temp_sensor_data'] = np.zeros([header['num_temp_sensor_channels'], num_supply_voltage_samples], dtype=np.uint)\n",
    "        data['board_adc_data'] = np.zeros([header['num_board_adc_channels'], num_board_adc_samples], dtype=np.uint)\n",
    "        \n",
    "        # by default, this script interprets digital events (digital inputs and outputs) as booleans\n",
    "        # if unsigned int values are preferred(0 for False, 1 for True), replace the 'dtype=np.bool' argument with 'dtype=np.uint' as shown\n",
    "        # the commented line below illustrates this for digital input data; the same can be done for digital out\n",
    "        \n",
    "        #data['board_dig_in_data'] = np.zeros([header['num_board_dig_in_channels'], num_board_dig_in_samples], dtype=np.uint)\n",
    "        data['board_dig_in_data'] = np.zeros([header['num_board_dig_in_channels'], num_board_dig_in_samples], dtype=np.bool)\n",
    "        data['board_dig_in_raw'] = np.zeros(num_board_dig_in_samples, dtype=np.uint)\n",
    "        \n",
    "        data['board_dig_out_data'] = np.zeros([header['num_board_dig_out_channels'], num_board_dig_out_samples], dtype=np.bool)\n",
    "        data['board_dig_out_raw'] = np.zeros(num_board_dig_out_samples, dtype=np.uint)\n",
    "\n",
    "        # Read sampled data from file.\n",
    "        print('Reading data from file...')\n",
    "\n",
    "        # Initialize indices used in looping\n",
    "        indices = {}\n",
    "        indices['amplifier'] = 0\n",
    "        indices['aux_input'] = 0\n",
    "        indices['supply_voltage'] = 0\n",
    "        indices['board_adc'] = 0\n",
    "        indices['board_dig_in'] = 0\n",
    "        indices['board_dig_out'] = 0\n",
    "\n",
    "        print_increment = 10\n",
    "        percent_done = print_increment\n",
    "        for i in range(num_data_blocks):\n",
    "            read_one_data_block(data, header, indices, fid)\n",
    "\n",
    "            # Increment indices\n",
    "            indices['amplifier'] += header['num_samples_per_data_block']\n",
    "            indices['aux_input'] += int(header['num_samples_per_data_block'] / 4)\n",
    "            indices['supply_voltage'] += 1\n",
    "            indices['board_adc'] += header['num_samples_per_data_block']\n",
    "            indices['board_dig_in'] += header['num_samples_per_data_block']\n",
    "            indices['board_dig_out'] += header['num_samples_per_data_block']            \n",
    "\n",
    "            fraction_done = 100 * (1.0 * i / num_data_blocks)\n",
    "            if fraction_done >= percent_done:\n",
    "                print('{}% done...'.format(percent_done))\n",
    "                percent_done = percent_done + print_increment\n",
    "\n",
    "        # Make sure we have read exactly the right amount of data.\n",
    "        bytes_remaining = filesize - fid.tell()\n",
    "        if bytes_remaining != 0: raise Exception('Error: End of file not reached.')\n",
    "\n",
    "    # Close data file.\n",
    "    fid.close()\n",
    "\n",
    "    if (data_present):\n",
    "        print('Parsing data...')\n",
    "\n",
    "        # Extract digital input channels to separate variables.\n",
    "        for i in range(header['num_board_dig_in_channels']):\n",
    "            data['board_dig_in_data'][i, :] = np.not_equal(np.bitwise_and(data['board_dig_in_raw'], (1 << header['board_dig_in_channels'][i]['native_order'])), 0)\n",
    "\n",
    "        # Extract digital output channels to separate variables.\n",
    "        for i in range(header['num_board_dig_out_channels']):\n",
    "            data['board_dig_out_data'][i, :] = np.not_equal(np.bitwise_and(data['board_dig_out_raw'], (1 << header['board_dig_out_channels'][i]['native_order'])), 0)\n",
    "\n",
    "        # Scale voltage levels appropriately.\n",
    "        data['amplifier_data'] = np.multiply(0.195, (data['amplifier_data'].astype(np.int32) - 32768))      # units = microvolts\n",
    "        data['aux_input_data'] = np.multiply(37.4e-6, data['aux_input_data'])               # units = volts\n",
    "        data['supply_voltage_data'] = np.multiply(74.8e-6, data['supply_voltage_data'])     # units = volts\n",
    "        if header['eval_board_mode'] == 1:\n",
    "            data['board_adc_data'] = np.multiply(152.59e-6, (data['board_adc_data'].astype(np.int32) - 32768)) # units = volts\n",
    "        elif header['eval_board_mode'] == 13:\n",
    "            data['board_adc_data'] = np.multiply(312.5e-6, (data['board_adc_data'].astype(np.int32) - 32768)) # units = volts\n",
    "        else:\n",
    "            data['board_adc_data'] = np.multiply(50.354e-6, data['board_adc_data'])           # units = volts\n",
    "        data['temp_sensor_data'] = np.multiply(0.01, data['temp_sensor_data'])               # units = deg C\n",
    "\n",
    "        # Check for gaps in timestamps.\n",
    "        num_gaps = np.sum(np.not_equal(data['t_amplifier'][1:]-data['t_amplifier'][:-1], 1))\n",
    "        if num_gaps == 0:\n",
    "            print('No missing timestamps in data.')\n",
    "        else:\n",
    "            print('Warning: {0} gaps in timestamp data found.  Time scale will not be uniform!'.format(num_gaps))\n",
    "\n",
    "        # Scale time steps (units = seconds).\n",
    "        data['t_amplifier'] = data['t_amplifier'] / header['sample_rate']\n",
    "        data['t_aux_input'] = data['t_amplifier'][range(0, len(data['t_amplifier']), 4)]\n",
    "        data['t_supply_voltage'] = data['t_amplifier'][range(0, len(data['t_amplifier']), header['num_samples_per_data_block'])]\n",
    "        data['t_board_adc'] = data['t_amplifier']\n",
    "        data['t_dig'] = data['t_amplifier']\n",
    "        data['t_temp_sensor'] = data['t_supply_voltage']\n",
    "\n",
    "        # If the software notch filter was selected during the recording, apply the\n",
    "        # same notch filter to amplifier data here.\n",
    "        if header['notch_filter_frequency'] > 0 and header['version']['major'] < 3:\n",
    "            print('Applying notch filter...')\n",
    "\n",
    "            print_increment = 10\n",
    "            percent_done = print_increment\n",
    "            for i in range(header['num_amplifier_channels']):\n",
    "                data['amplifier_data'][i,:] = notch_filter(data['amplifier_data'][i,:], header['sample_rate'], header['notch_filter_frequency'], 10)\n",
    "\n",
    "                fraction_done = 100 * (i / header['num_amplifier_channels'])\n",
    "                if fraction_done >= percent_done:\n",
    "                    print('{}% done...'.format(percent_done))\n",
    "                    percent_done += print_increment\n",
    "    else:\n",
    "        data = [];\n",
    "\n",
    "    # Move variables to result struct.\n",
    "    result = data_to_result(header, data, data_present)\n",
    "\n",
    "    print('Done!  Elapsed time: {0:0.1f} seconds'.format(time.time() - tic))\n",
    "    return result\n",
    "\n",
    "def plural(n):\n",
    "    \"\"\"Utility function to optionally pluralize words based on the value of n.\n",
    "    \"\"\"\n",
    "\n",
    "    if n == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return 's'\n",
    "\n",
    "def sorting_day_split(sorting, date_id_all, day_length, pack_folder, sorting_save_name='firings_inlier'):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    sampling_freq = sorting.get_sampling_frequency()\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,40))\n",
    "    sw.plot_rasters(sorting,  time_range=(0, np.sum(day_length)),ax=ax)\n",
    "\n",
    "    colors = []\n",
    "    cm = pylab.get_cmap('rainbow')\n",
    "    NUM_COLORS = len(day_length)\n",
    "    for i in range(NUM_COLORS):\n",
    "        colors.append(cm(1. * i / NUM_COLORS))  # color will now be an RGBA tuple\n",
    "    \n",
    "    for i in range(len(day_length)):\n",
    "        ax.axvspan(np.sum(day_length[:i])/sampling_freq, np.sum(day_length[:(i+1)])/sampling_freq, \n",
    "                   facecolor=colors[i], alpha=0.1)\n",
    "    \n",
    "    plt.savefig(waveform_folder+'/rasters.pdf',dpi=300)   \n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(len(day_length)):\n",
    "        pack_folder_i = pack_folder + '/' + date_id_all[i] + '/'\n",
    "\n",
    "        if os.path.exists(pack_folder_i)==False:\n",
    "            os.mkdir(pack_folder_i)\n",
    "\n",
    "        start_frame = np.sum(day_length[:i])\n",
    "        end_frame = np.sum(day_length[:(i+1)])\n",
    "\n",
    "        sub_sorting = sorting.frame_slice(start_frame, end_frame)\n",
    "\n",
    "        keep_unit_ids = []\n",
    "        for unit_id in sub_sorting.unit_ids:\n",
    "            spike_train = sub_sorting.get_unit_spike_train(unit_id=unit_id)\n",
    "            n = spike_train.size\n",
    "            if(n>20):\n",
    "                keep_unit_ids.append(unit_id)\n",
    "\n",
    "        curated_sub_sorting = sub_sorting.select_units(unit_ids=keep_unit_ids, renamed_unit_ids=None)\n",
    "\n",
    "        save_path = pack_folder_i + '/sorting/' + sorting_save_name + '.npz'\n",
    "        if os.path.exists(pack_folder_i + '/sorting/')==False:\n",
    "            os.mkdir(pack_folder_i + '/sorting/')\n",
    "        NpzSortingExtractor.write_sorting(curated_sub_sorting, save_path)\n",
    "\n",
    "def create_mesh_probe(n):\n",
    "    \"\"\"\n",
    "    positions=np.array([[80,0],[80,80],[80,160],[80,240],[0,0],\n",
    "                         [0,80],[0,160],[0,240],[0,320],[0,400],\n",
    "                         [0,480],[0,560],[80,320],[80,400],[80,480],\n",
    "                         [80,560],[160,560],[160,480],[160,400],[160,320],\n",
    "                        [240,560],[240,480],[240,400],[240,320],[240,240],\n",
    "                         [240,160],[240,80],[240,0],[160,240],[160,160],\n",
    "                       [160,80],[160,0]])\n",
    "                       \"\"\"\n",
    "    \n",
    "    #distribution ch 16-48\n",
    "    positions=np.array([[160,0],[80,0],[160,80],[80,80],[160,160],\n",
    "                         [80,160],[160,240],[80,240],[160,320],[80,320],\n",
    "                         [160,400],[80,400],[160,480],[80,480],[160,560],\n",
    "                         [80,560],[240,0],[0,0],[240,80],[0,80],\n",
    "                         [240,160],[0,160],[240,240],[0,240],[240,320],\n",
    "                         [0,320],[240,400],[0,400],[240,480],[0,480],\n",
    "                         [240,560],[0,560]])\n",
    "    \n",
    "    mesh_probe = Probe(ndim=2, si_units='um')\n",
    "    mesh_probe.set_contacts(positions=positions, shapes='circle', shape_params={'radius': 5})\n",
    "\n",
    "    ant = {'first_index':0}\n",
    "    mesh_probe.annotate(**ant)\n",
    "    channel_indices_raw = np.arange(n)\n",
    "    channel_indices = [i for i in channel_indices_raw]\n",
    "    mesh_probe.set_device_channel_indices(channel_indices)\n",
    "    return mesh_probe\n",
    "\n",
    "def stack_recordings(pack_folder_pre, mesh_probe, trigger_val=3.5):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    cont_data_all = []\n",
    "    \n",
    "    for dirpath, dirname, filenames in os.walk(pack_folder_pre):\n",
    "        for i in filenames:\n",
    "            if '.rhd' in i:\n",
    "                print(dirpath+'/'+i)\n",
    "                raw_data = read_data(dirpath+'/'+i)\n",
    "                sampling_freq = raw_data['frequency_parameters']['amplifier_sample_rate']\n",
    "                cont_data_all.append(raw_data['amplifier_data'].T)\n",
    "\n",
    "    cont_data_all = np.vstack(cont_data_all)\n",
    "    recording = se.NumpyRecording(traces_list=cont_data_all, sampling_frequency=sampling_freq)\n",
    "    recording.set_probe(mesh_probe, in_place=True)\n",
    "\n",
    "    return recording, cont_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15bfab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mesh_probe = create_mesh_probe(32)\n",
    "plot_probe(mesh_probe, with_channel_index=True)\n",
    "plot_probe(mesh_probe, with_device_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef86cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Conduct Spike Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831893ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set parameters\"\"\"\n",
    "\n",
    "date_id_all = ['043023_axolotl_B_1', '050123_axolotl_B_1', '050223_axolotl_B_1', '050323_axolotl_B_1', '050423_axolotl_B_1']\n",
    "\n",
    "recording_traces = []\n",
    "session_length_concat = []\n",
    "day_length = []\n",
    "cont_trigger_all_all = []\n",
    "save_folder_name = '_'.join(date_id_all)\n",
    "data_folder_all = f'./processed_data/Ephys_concat_{save_folder_name}/'\n",
    "\n",
    "sorting_method=\"mountainsort\"\n",
    "\n",
    "sorting_save_path = data_folder_all + sorting_method + '/'\n",
    "pack_folder = sorting_save_path\n",
    "\n",
    "output_folder = sorting_save_path + '/sorting'\n",
    "firing_save_path = output_folder + f'/firings.npz'\n",
    "\n",
    "freq_max = 3000\n",
    "freq_min = 300\n",
    "fs = 10000\n",
    "default_params = {\n",
    "        'detect_sign': -1,  # Use -1, 0, or 1, depending on the sign of the spikes in the recording\n",
    "        'adjacency_radius': -1,  # Use -1 to include all channels in every neighborhood\n",
    "        'freq_min': 300,  # Use None for no bandpass filtering\n",
    "        'freq_max': 3000,\n",
    "        'filter': True,\n",
    "        'whiten': True,  # Whether to do channel whitening as part of preprocessing\n",
    "        # 'curation': False,\n",
    "        # 'num_workers': None,\n",
    "        'num_workers': 9,\n",
    "        'clip_size': 50,\n",
    "        'detect_threshold': 5, # 5\n",
    "        'detect_interval': 30,  # Minimum number of timepoints between events detected on the same channel, 30\n",
    "        # 'noise_overlap_threshold': None,  # Use None for no automated curation'\n",
    "    }\n",
    "\n",
    "if sorting_method == 'waveclus':\n",
    "    default_TDC_params = ss.WaveClusSorter.default_params()\n",
    "    default_TDC_params['detect_threshold']=5\n",
    "    pprint(default_TDC_params)\n",
    "if sorting_method == 'kilosort3':\n",
    "    default_TDC_params = ss.Kilosort3Sorter.default_params()\n",
    "    pprint(default_TDC_params)\n",
    "if sorting_method == 'ironclust':\n",
    "    default_TDC_params = ss.IronClustSorter.default_params()\n",
    "    pprint(default_TDC_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c34c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load Recordings\"\"\"\n",
    "\n",
    "if os.path.exists(data_folder_all+'recordings/'):\n",
    "    print('data_folder_all already exists')\n",
    "    recording_concat = spikeinterface.core.base.BaseExtractor.load_from_folder(data_folder_all+'recordings/')\n",
    "    session_length_concat = np.load(data_folder_all+'recordings/session_length.npy')\n",
    "    day_length=np.load(data_folder_all+'recordings/day_length.npy')\n",
    "\n",
    "else:\n",
    "    for date_id in date_id_all:\n",
    "        pack_folder_pre =  f'/home/jialiulab/disk1/yichun/hao_sheng/data/cyborg_axolotl/{date_id}' # Source data folder (flipped?)\n",
    "        data_folder_pre = data_folder_all + date_id + '/recordings/' # Output folder\n",
    "        \n",
    "        # If output folder exists, just load recording object from there\n",
    "        if os.path.exists(data_folder_pre):\n",
    "            recording = spikeinterface.core.base.BaseExtractor.load_from_folder(data_folder_pre)\n",
    "            session_length = np.load(data_folder_pre + 'session_length.npy')\n",
    "        \n",
    "        # Otherwise, read in .rhd file, create recording object, and save down recording object\n",
    "        else:\n",
    "            mesh_probe = create_mesh_probe(32)\n",
    "            recording, session_length = stack_recordings(pack_folder_pre, mesh_probe, trigger_val=3)\n",
    "            recording.set_probe(mesh_probe, in_place=True)\n",
    "            recording = recording.save(folder=data_folder_pre)\n",
    "            np.save(data_folder_pre+'session_length.npy', session_length)\n",
    "        \n",
    "        sampling_freq = recording.get_sampling_frequency()\n",
    "        recording_trace = recording.get_traces()\n",
    "        recording_traces.append(recording_trace)\n",
    "        session_length_concat.append(session_length)\n",
    "        print(recording_trace.shape)\n",
    "        day_length.append(recording_trace.shape[0])\n",
    "\n",
    "    recording_traces = np.vstack(recording_traces)\n",
    "    session_length_concat = np.vstack(session_length_concat)\n",
    "\n",
    "    recording_concat = se.NumpyRecording(traces_list=recording_traces, sampling_frequency=sampling_freq)\n",
    "    recording_concat.set_probe(mesh_probe, in_place=True)\n",
    "    recording_concat = recording_concat.save(folder = data_folder_all + 'recordings/')\n",
    "\n",
    "    np.save(data_folder_all + 'recordings/session_length.npy', session_length_concat)\n",
    "    np.save(data_folder_all + 'recordings/day_length.npy', day_length)\n",
    "    \n",
    "print(recording_concat)\n",
    "print('Num. channels = {}'.format(len(recording_concat.get_channel_ids())))\n",
    "print('Sampling frequency = {} Hz'.format(recording_concat.get_sampling_frequency()))\n",
    "print('Num. timepoints seg0= {}'.format(recording_concat.get_num_segments()))\n",
    "\n",
    "if os.path.exists(sorting_save_path)==False:\n",
    "    print(sorting_save_path)\n",
    "    os.mkdir(sorting_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_f = st.preprocessing.bandpass_filter(recording_concat, freq_min=freq_min, freq_max=freq_max)\n",
    "recording_cmr = st.preprocessing.common_reference(recording_f, reference='global',operator='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8a01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(firing_save_path):\n",
    "    print(0)\n",
    "    sorting_wave_clus = ss.run_sorter(sorter_name='mountainsort4', \n",
    "                                      recording=recording_cmr, \n",
    "                                      remove_existing_folder='True', \n",
    "                                      output_folder=output_folder, \n",
    "                                      **default_params,)        \n",
    "    keep_unit_ids = []\n",
    "    for unit_id in sorting_wave_clus.unit_ids:\n",
    "        spike_train = sorting_wave_clus.get_unit_spike_train(unit_id=unit_id)\n",
    "        n = spike_train.size\n",
    "        if(n>20):\n",
    "            keep_unit_ids.append(unit_id)\n",
    "\n",
    "    sorting = sorting_wave_clus.select_units(unit_ids=keep_unit_ids, renamed_unit_ids=None)\n",
    "    NpzSortingExtractor.write_sorting(sorting, firing_save_path)\n",
    "\n",
    "sorting = se.NpzSortingExtractor(firing_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4fc05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extract and show waveforms (all days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_unit_show(we, recording_cmr, sorting, pack_folder, waveform_type):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "\n",
    "    # plot_probe(mesh_probe,with_channel_index=True)\n",
    "    \n",
    "    waveform_folder = pack_folder + 'waveforms/'\n",
    "    \n",
    "    fig, axs = plt.subplots(int(np.ceil(len(sorting.unit_ids)/4)), 4, figsize=(20, 5*np.ceil(len(sorting.unit_ids)/4)))\n",
    "    sw.plot_unit_templates(we, unit_ids=sorting.unit_ids, axes=axs)\n",
    "    plt.savefig(waveform_folder+'/templates' + waveform_type + '.pdf',dpi=600)\n",
    "        \n",
    "    # fig, axs = plt.subplots(int(np.ceil(len(sorting.unit_ids)/4)), 4, figsize=(20, 5*np.ceil(len(sorting.unit_ids)/4)))\n",
    "    # sw.plot_unit_probe_map(we, unit_ids=sorting.unit_ids,\n",
    "    #                   axes=axs)\n",
    "    # plt.savefig(waveform_folder+'/probe_map.pdf',dpi=300)\n",
    "        \n",
    "    extremum_channels_ids = st.get_template_extremum_channel(we, peak_sign='neg')\n",
    "    \n",
    "    colors=[]\n",
    "    cm = get_cmap('rainbow')\n",
    "    NUM_COLORS = len(sorting.unit_ids)\n",
    "    for i in range(NUM_COLORS):\n",
    "        colors.append(cm(1. * i / NUM_COLORS))  # color will now be an RGBA tuple\n",
    "\n",
    "    # for i, unit_id in enumerate(sorting.unit_ids):\n",
    "    #     template = we.get_waveforms(unit_id)\n",
    "    #     ax = axs[int(np.floor(i/4)), int(np.mod(i,4))]\n",
    "    #     ax.plot(template[:,:, extremum_channels_ids[unit_id]].T, lw=0.3,label=unit_id,color=colors[i])\n",
    "    #     ax.set_title(f'template{unit_id}')\n",
    "    #     break\n",
    "\n",
    "    # plt.savefig(waveform_folder+'/extremum_waveforms_map.pdf',dpi=300)\n",
    "    \n",
    "    fig, axs = plt.subplots(int(np.ceil(len(sorting.unit_ids)/4)), 4, figsize=(20, 5*np.ceil(len(sorting.unit_ids)/4)))\n",
    "\n",
    "    for i, unit_id in enumerate(sorting.unit_ids):\n",
    "        if int(np.ceil(len(sorting.unit_ids)/4))>1:\n",
    "            template = we.get_template(unit_id)\n",
    "            ax = axs[int(np.floor(i/4)), int(np.mod(i,4))]\n",
    "            ax.plot(template[:, extremum_channels_ids[unit_id]].T, lw=3,label=unit_id,color=colors[i])\n",
    "            ax.set_title(f'template{unit_id}')\n",
    "        else:\n",
    "            template = we.get_template(unit_id)\n",
    "            ax = axs[int(np.mod(i,4))]\n",
    "            ax.plot(template[:, extremum_channels_ids[unit_id]].T, lw=3,label=unit_id,color=colors[i])\n",
    "            ax.set_title(f'template{unit_id}')\n",
    "    \n",
    "    plt.savefig(waveform_folder+'/waveform' + waveform_type + '.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22409140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveform_folder = pack_folder + 'waveforms/'\n",
    "\n",
    "we = spikeinterface.extract_waveforms(recording_cmr, sorting, waveform_folder, \n",
    "                                      load_if_exists=True, ms_before=1, ms_after=2., \n",
    "                                      max_spikes_per_unit=1000000, n_jobs=-1, chunk_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfef346",
   "metadata": {},
   "outputs": [],
   "source": [
    "we = we[0]\n",
    "we.recording.set_probe(mesh_probe, in_place=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a9089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_day_split(sorting, date_id_all, day_length, pack_folder, \n",
    "                  sorting_save_name='firings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc2ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(8,4,figsize=(15,10))\n",
    "sw.plot_isi_distribution(sorting, window_ms=200.0, bin_ms=1.0,axes=ax)\n",
    "plt.savefig(waveform_folder+'/ISI.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50466525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorting_unit_show(we, recording_cmr, sorting, pack_folder, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e089975",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_unit_ids_pack = []\n",
    "delete_unit_ids_pack = [3,5,9,21,22,24,25,26]\n",
    "\n",
    "we_load_if_exists = True\n",
    "waveform_show = False\n",
    "input_state = 'merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25808ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_save_folder = pack_folder + f'/curation_result_{input_state}/'\n",
    "\n",
    "if os.path.exists(curation_save_folder)==False:\n",
    "    os.mkdir(curation_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sorting._sorting_segments[0]\n",
    "merged_sorting = sorting\n",
    "remove_ids = []\n",
    "\n",
    "for idx in range(len(merge_unit_ids_pack)):\n",
    "    merge_unit_ids = merge_unit_ids_pack[idx]\n",
    "\n",
    "    for unit_id_id, unit_id in enumerate(merge_unit_ids):\n",
    "        S.spike_labels[S.spike_labels == unit_id] = merge_unit_ids[0]\n",
    "\n",
    "    merged_sorting._sorting_segments[0] = S\n",
    "    remove_ids.extend(merge_unit_ids[1:])\n",
    "\n",
    "remove_ids+=delete_unit_ids_pack\n",
    "keep_ids = merged_sorting.unit_ids[~np.isin(merged_sorting.unit_ids, remove_ids)]\n",
    "\n",
    "merged_sorting = merged_sorting.select_units(unit_ids=keep_ids, renamed_unit_ids=None)    \n",
    "\n",
    "waveform_save_folder = pack_folder + '/waveforms_merged'\n",
    "if(we_load_if_exists == False):\n",
    "    if os.path.exists(waveform_save_folder):\n",
    "        shutil.rmtree(waveform_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = pack_folder + f'/sorting/firings_merged.npz'\n",
    "NpzSortingExtractor.write_sorting(merged_sorting, save_path)\n",
    "\n",
    "merged_sorting = se.NpzSortingExtractor(save_path)\n",
    "\n",
    "merged_we = spikeinterface.extract_waveforms(recording_cmr, merged_sorting, waveform_save_folder, \n",
    "                                             load_if_exists=we_load_if_exists, overwrite=False,ms_before=1, ms_after=2, \n",
    "                                             max_spikes_per_unit=1000000, n_jobs=1, chunk_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = merged_sorting\n",
    "we = merged_we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26956950",
   "metadata": {},
   "outputs": [],
   "source": [
    "we=we[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414d69d-0fa2-495b-9a20-a871cef74236",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Analysis (All Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443971b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_day_split(sorting, date_id_all, day_length, pack_folder, \n",
    "                  sorting_save_name='firings_merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15d2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(5,4,figsize=(15,12))\n",
    "sw.plot_isi_distribution(sorting, window_ms=200.0, bin_ms=2.0,axes=ax)\n",
    "plt.savefig(waveform_folder+'/ISI.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ff477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_unit_show(we, recording_cmr, sorting, pack_folder, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06179c3a-1776-46f0-8bfe-8c6022646ad9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## By Electrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5be1ef-c057-4ecb-8283-69a71d351a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extremum_channels_ids = st.get_template_extremum_channel(we, peak_sign='neg')\n",
    "probe_groups = np.arange(0,32)\n",
    "NumShanks = 32\n",
    "\n",
    "recording_cmr.set_property('group', probe_groups, ids=None)\n",
    "slice_recording = recording_cmr.split_by()\n",
    "slice_sorting = []\n",
    "slice_we = []\n",
    "slice_unit_ids = []\n",
    "\n",
    "for shank_id in np.arange(NumShanks):\n",
    "    shank_channel_ids = np.where(probe_groups==shank_id)[0]\n",
    "    shank_unit_ids = sorting.unit_ids[np.where(np.isin(np.array(list(extremum_channels_ids.values())), shank_channel_ids))[0]]\n",
    "\n",
    "    print(f'Electrode:{shank_id+1}, units: {shank_unit_ids}')\n",
    "    slice_unit_ids.append(shank_unit_ids)\n",
    "\n",
    "    shank_sorting = sorting.select_units(unit_ids=shank_unit_ids, renamed_unit_ids=None)\n",
    "    slice_sorting.append(shank_sorting)\n",
    "\n",
    "    shank_waveform_folder = pack_folder + f'/waveforms_electrode{shank_id+1}'\n",
    "    if(we_load_if_exists == False):\n",
    "        if os.path.exists(shank_waveform_folder):\n",
    "            shutil.rmtree(shank_waveform_folder)\n",
    " \n",
    "    shank_recording = slice_recording[shank_id]\n",
    "    shank_we = spikeinterface.extract_waveforms(shank_recording, shank_sorting, shank_waveform_folder,\n",
    "        load_if_exists=we_load_if_exists,\n",
    "        ms_before=1, ms_after=2., max_spikes_per_unit=1000000,\n",
    "        n_jobs=1, chunk_size=30000)\n",
    "    \n",
    "    slice_we.append(shank_we)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c316d-cfac-4168-9a59-d024c30d2d20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Analysis (By Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec75b9-d9b7-440c-bb1e-1a78e51c92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Curation by Day\"\"\"\n",
    "\n",
    "we_load_if_exists = True\n",
    "\n",
    "print(sorting)\n",
    "print(recording_cmr)\n",
    "for day_id in range(len(date_id_all)):\n",
    "    \n",
    "    data_folder_day = data_folder_all + date_id_all[day_id] + '/'\n",
    "    pack_folder_day = pack_folder + date_id_all[day_id] + '/'\n",
    "    if os.path.exists(data_folder_day)==False:\n",
    "        os.mkdir(data_folder_day)\n",
    "    \n",
    "    curation_save_folder_day = pack_folder_day + 'curation_result_merged/'\n",
    "    if os.path.exists(curation_save_folder_day)==False:\n",
    "        os.mkdir(curation_save_folder_day)\n",
    "    \n",
    "    # Load sorting object\n",
    "    firing_save_path_day = pack_folder_day + 'sorting/firings_merged.npz'\n",
    "    sorting_day = se.NpzSortingExtractor(firing_save_path_day)\n",
    "    print(sorting_day)\n",
    "    \n",
    "    # Load recording object\n",
    "    recording_save_path_day = data_folder_day + 'recordings/'\n",
    "    recording_day = spikeinterface.core.base.BaseExtractor.load_from_folder(recording_save_path_day)\n",
    "    recording_f_day = st.preprocessing.bandpass_filter(recording_day, freq_min=freq_min, freq_max=freq_max)\n",
    "    recording_cmr_day = st.preprocessing.common_reference(recording_f_day, reference='global', operator='average')\n",
    "    print(recording_cmr_day)\n",
    "    \n",
    "    # Save down waveform object\n",
    "    waveform_save_folder_day = pack_folder_day + 'waveforms_merged/'\n",
    "    if(we_load_if_exists == False):\n",
    "        if os.path.exists(waveform_save_folder_day):\n",
    "            shutil.rmtree(waveform_save_folder_day)\n",
    "    \n",
    "    we_day = spikeinterface.extract_waveforms(recording_cmr_day, sorting_day, waveform_save_folder_day, \n",
    "                                              load_if_exists=we_load_if_exists, overwrite=False, ms_before=1, ms_after=2, \n",
    "                                              max_spikes_per_unit=1000000, n_jobs=1, chunk_size=30000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefce29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Plot autocorrelogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311392df-0672-4517-8701-3f796af757b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Autocorrelogram with 20ms window\"\"\"\n",
    "\n",
    "corr_bad_units = shank_autocorrelogram_show(slice_sorting, window_ms=20.0, bin_ms=0.1, threshold_ms=1, \n",
    "                                            symmetrize=True, neuron_id_rename=False, save_path=curation_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67c647-aa40-4e05-ae94-5839bce1c335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Autocorrelogram with 100ms window\"\"\"\n",
    "\n",
    "corr_bad_units = shank_autocorrelogram_show(slice_sorting, window_ms=100.0, bin_ms=0.1, threshold_ms=1, \n",
    "                                            symmetrize=True, neuron_id_rename=False, save_path=curation_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75a717-f834-4b9c-a022-16522f880161",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Plot correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c131a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sw.plot_autocorrelograms(sorting, sorting.unit_ids)\n",
    "plt.savefig(waveform_folder+'/autocorrelogram.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312143b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.plot_crosscorrelograms(sorting, sorting.unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba186ae-ffd2-4838-af48-78e59f3dae66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shank_correlogram_show([sorting], window_ms=200.0, bin_ms=1.0, threshold_ms=3, symmetrize=True, neuron_id_rename=False, save_path=curation_save_folder)\n",
    "\n",
    "# window_ms = 100, 100, 50\n",
    "# bin_ms = 1.0\n",
    "# threshold_ms = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8707c-3495-411a-b977-86e54140a203",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe06e61-af96-41f5-b14f-a51dbf34d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_cal(info, degree=2):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \n",
    "    Parameters\n",
    "    --------------------------\n",
    "    info: pandas dataframe with neuron_id, template\n",
    "    degree:\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    --------------------------\n",
    "    np.array(location_day): an array of the x and y positions of each unit\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_positions = np.array([[160,0],[80,0],[160,80],[80,80],[160,160],\n",
    "                                 [80,160],[160,240],[80,240],[160,320],[80,320],\n",
    "                                 [160,400],[80,400],[160,480],[80,480],[160,560],\n",
    "                                 [80,560],[240,0],[0,0],[240,80],[0,80],\n",
    "                                 [240,160],[0,160],[240,240],[0,240],[240,320],\n",
    "                                 [0,320],[240,400],[0,400],[240,480],[0,480],\n",
    "                                 [240,560],[0,560]])\n",
    "\n",
    "    sensor_channels = np.arange(32)\n",
    "    location_day = []\n",
    "\n",
    "    for neuron_id in range(len(info)):\n",
    "        \n",
    "        info_unit = info.iloc[neuron_id] # gets the pd data for neuron_id\n",
    "        template = info_unit['template'][:, sensor_channels] # gets the template at channel ids for the shank where neuron_id is located\n",
    "        \n",
    "        NumChannels = template.shape[1] # number of channels in the shank\n",
    "        amplitudes = np.max(template,axis=0) - np.min(template,axis=0) # the peak-to-trough of the template at each channel\n",
    "        \n",
    "        x = np.sum(np.array([sensor_positions[i,0]*(amplitudes[i]**degree) for i in range(NumChannels)]))\n",
    "        x /= np.sum(np.array([(amplitudes[i]**degree) for i in range(NumChannels)]))\n",
    "        y = np.sum(np.array([sensor_positions[i,1]*(amplitudes[i]**degree) for i in range(NumChannels)]))\n",
    "        y /= np.sum(np.array([(amplitudes[i]**degree) for i in range(NumChannels)]))\n",
    "\n",
    "        location_day.append([x,y])\n",
    "        \n",
    "    return np.array(location_day)\n",
    "\n",
    "def unit_position_plot(location_day0, location_day1, day0_name, day_follow_name, with_device_index=True, \n",
    "                       degree=2, colors = ['darkblue','red'], s=[100,80], linewidth=1, save_folder='./'):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_channels = np.arange(32)    \n",
    "    ShankNum = len(sensor_channels)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 6))\n",
    "\n",
    "    mesh_probe = create_mesh_probe(32)\n",
    "        \n",
    "    plot_probe(mesh_probe, with_device_index=with_device_index, ax=ax)\n",
    "    location_day0 = location_day0\n",
    "    location_day1 = location_day1\n",
    "\n",
    "    for neuron_id in range(location_day0.shape[0]):\n",
    "        label = f'neuron{neuron_id+1}'\n",
    "        ax.scatter(location_day0[neuron_id, 0], location_day0[neuron_id, 1], marker='.', s=s[0], color=colors[0], label=label)\n",
    "    \n",
    "    for neuron_id in range(location_day0.shape[0]):\n",
    "        label = f'neuron{neuron_id+1}'\n",
    "        ax.scatter(location_day1[neuron_id, 0], location_day1[neuron_id, 1], marker='.', s=s[1], color=colors[1], label=label)\n",
    "        ax.plot([location_day0[neuron_id, 0], location_day1[neuron_id, 0]], [location_day0[neuron_id, 1], location_day1[neuron_id, 1]], c=colors[1])\n",
    "         \n",
    "    fig.suptitle(f'{day0_name} - {day_follow_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_folder+f'Displacement_{day0_name}_{day_follow_name}_degree{degree}.pdf')\n",
    "\n",
    "def unit_position_plot_oneday(location, day_name, unit_ids, with_device_index=True, color='gray',\n",
    "                              s=[100,80], linewidth=1, save_folder='./'):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_channels = np.arange(32)    \n",
    "    ShankNum = len(sensor_channels)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 6))\n",
    "\n",
    "    mesh_probe = create_mesh_probe(32)\n",
    "        \n",
    "    plot_probe(mesh_probe, with_device_index=with_device_index, ax=ax)\n",
    "\n",
    "    for neuron_id in range(len(unit_ids)):\n",
    "        label = f'neuron{neuron_id+1}'\n",
    "        ax.scatter(location[neuron_id, 0], location[neuron_id, 1], marker='.', s=s[0], color=color, label=label)\n",
    "        ax.annotate(unit_ids[neuron_id], (location[neuron_id, 0], location[neuron_id, 1]))\n",
    "    \n",
    "    fig.suptitle(f'{day_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_folder+f'Unit_position_{day_name}.pdf')\n",
    "    \n",
    "def geometry_drift_plot(location_day0, location_day1, day0_name, day1_name, degree=2, levels=5,thresh=.3,\n",
    "                        colors=['darkblue','red'], s=[100,80], lim=[-15,15], save_folder='./'):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    location = {}\n",
    "    location['drift_x'] = location_day1[:,0] - location_day0[:,0]\n",
    "    location['drift_y'] = location_day1[:,1] - location_day0[:,1]\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.kdeplot(data=location, x=f'drift_x', y=f'drift_y', levels=levels, ax=ax,\n",
    "                thresh=thresh, color='gray',zorder=-1)\n",
    "    dift_vec =[[0,location[f'drift_x'].mean()],\n",
    "              [0,location[f'drift_y'].mean()]]\n",
    "    ax.plot(dift_vec[0],dift_vec[1],color=colors[1],zorder=1,linewidth=2)\n",
    "    ax.scatter([0],[0],color=colors[0],zorder=2,s=s[0])\n",
    "    ax.scatter([location[f'drift_x'].mean()],[location[f'drift_y'].mean()],color=colors[1],zorder=3,s=s[1])\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(lim)\n",
    "    ax.set_title(f'geometry drift: {day0_name}-{day1_name}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(save_folder+f'geometry_drift_{day0_name}_{day1_name}_degree{degree}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f8783-511e-496e-bc8d-85d66066de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create info file\"\"\"\n",
    "\n",
    "days = ['043023', '050123', '050223', '050323', '050423']\n",
    "\n",
    "day_name_all = []\n",
    "day_id_all = []\n",
    "unit_id_all = []\n",
    "template_all = []\n",
    "waveform_all = []\n",
    "\n",
    "for day_id, day_name in enumerate(date_id_all):\n",
    "    \n",
    "    data_folder_day = data_folder_all + day_name + '/'\n",
    "    pack_folder_day = pack_folder + day_name + '/'\n",
    "    slice_curated_ids = [np.nan]*NumShanks\n",
    "    shank_ids = []\n",
    "    \n",
    "    \"\"\"\n",
    "    for shank_id in range(NumShanks):\n",
    "        shank_curated_waveform_folder = pack_folder + f'/waveforms_electrode{shank_id+1}'\n",
    "        if os.path.exists(shank_curated_waveform_folder)==True:\n",
    "            slice_curated_we = WaveformExtractor.load_from_folder(shank_curated_waveform_folder)\n",
    "            slice_curated_ids[shank_id] = slice_curated_we.sorting.unit_ids\n",
    "            slice_shank_ids = shank_id*np.ones((len(slice_curated_ids[shank_id]),)).astype(int)\n",
    "            shank_ids.append(slice_shank_ids)\n",
    "\n",
    "    shank_ids = np.hstack(shank_ids)\n",
    "    curated_ids = np.hstack(slice_curated_ids)\n",
    "    curated_ids = curated_ids[~np.isnan(curated_ids)]\n",
    "    \"\"\"\n",
    "    \n",
    "    waveform_folder_day = pack_folder_day + 'waveforms_merged/'\n",
    "    we_day = WaveformExtractor.load_from_folder(waveform_folder_day)\n",
    "    curated_ids_day = we_day.sorting.unit_ids\n",
    "    template_day = we_day.get_all_templates(unit_ids=curated_ids_day)\n",
    "    \n",
    "    for idx, unit_id in enumerate(curated_ids_day):\n",
    "\n",
    "        day_name_all.append(day_name)\n",
    "        day_id_all.append(day_id)\n",
    "        unit_id_all.append(unit_id)\n",
    "        template_all.append(template_day[idx,:,:])\n",
    "        waveform_all.append(we_day.get_waveforms(unit_id=unit_id))\n",
    "\n",
    "info = {'day_name': day_name_all, 'day_id': day_id_all, 'unit_id': unit_id_all, 'template': template_all, 'waveform': waveform_all}\n",
    "info = pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9a851-89c6-4513-9e7a-16d8b60ee1e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create displacement plots\"\"\"\n",
    "\n",
    "for day0_id in range(len(date_id_all)-1):\n",
    "    \n",
    "    day0_name = date_id_all[day0_id]\n",
    "    info_day0 = info.loc[info['day_id']==day0_id]\n",
    "    \n",
    "    unit_ids_day0 = info_day0['unit_id'].values # Holds the unit ids for day0\n",
    "    template_day0 = info_day0['template'].values # Holds the unit templates for day0\n",
    "    locations_day0 = location_cal(info_day0) # Calculates the location for day0\n",
    "    print(day0_name)\n",
    "    \n",
    "    for day_follow_id in np.arange(day0_id, len(date_id_all)):\n",
    "        \n",
    "        day_follow_name = date_id_all[day_follow_id]\n",
    "        info_day_follow = info.loc[info['day_id']==day_follow_id]\n",
    "        \n",
    "        unit_ids_day_follow = info_day_follow['unit_id'].values # Holds the neuron ids for day_follow\n",
    "        template_day_follow = info_day_follow['template'].values # Holds the template for day_follow\n",
    "        locations_day_follow = location_cal(info_day_follow) # Calculates the location fo day_follow?\n",
    "        \n",
    "        stable_unit_ids = np.intersect1d(unit_ids_day0, unit_ids_day_follow) # the neuron_id that are found in both days\n",
    "        \n",
    "        indices_day0,_ = ismember(unit_ids_day0, stable_unit_ids) # finds stable neurons in day0 \n",
    "        indices_day_follow,_ = ismember(unit_ids_day_follow, stable_unit_ids) # finds stable neurons in day_follow\n",
    "        locations_day0_stable = locations_day0[indices_day0,:] # gets locations of stable neurons in day0\n",
    "        locations_day_follow_stable = locations_day_follow[indices_day_follow,:] # gets locations of stable neurons in day_follow\n",
    "\n",
    "        # Unit position plot\n",
    "        unit_position_plot(locations_day0_stable, locations_day_follow_stable,\n",
    "                           day0_name, day_follow_name, with_device_index=False, degree=2,\n",
    "                           colors=['darkblue','red'], s=[400,160], linewidth=2, \n",
    "                           save_folder=pack_folder)\n",
    "        \n",
    "        # Geometry drift plot\n",
    "        geometry_drift_plot(locations_day0_stable, locations_day_follow_stable, day0_name, day_follow_name, \n",
    "                            degree=2, colors=['darkblue','red'], s=[100,60], lim=[-400,200], levels=5, \n",
    "                            thresh=0.2, save_folder=pack_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b0d07-d56a-4336-bb77-9995dedbbc3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Unit Location (All Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522a7c7-67b0-430d-a709-4452beb447f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "day_name_concat = []\n",
    "day_id_concat = []\n",
    "unit_id_concat = []\n",
    "template_concat = []\n",
    "\n",
    "curated_ids = we.sorting.unit_ids\n",
    "curated_templates = we.get_all_templates(unit_ids=curated_ids)\n",
    "\n",
    "for idx, unit_id in enumerate(curated_ids):\n",
    "    day_name_concat.append('All_Days')\n",
    "    day_id_concat.append('All_Days')\n",
    "    unit_id_concat.append(unit_id)\n",
    "    template_concat.append(curated_templates[idx,:,:])\n",
    "\n",
    "info_concat = {'day_name': day_name_concat, 'day_id': day_id_concat, 'unit_id': unit_id_concat, 'template': template_concat}\n",
    "info_concat = pd.DataFrame(info_concat)\n",
    "\n",
    "\"\"\"Location plots for all days\"\"\"\n",
    "\n",
    "locations_concat = location_cal(info_concat)\n",
    "\n",
    "unit_position_plot_oneday(locations_concat, 'All_Days', unit_ids = unit_id_concat, with_device_index=False, \n",
    "                          color='gray', s=[400,160], linewidth=2, save_folder=pack_folder)\n",
    "\n",
    "\"\"\"Location plots for individual days\"\"\"\n",
    "\n",
    "for day_id, day_name in enumerate(date_id_all):\n",
    "    \n",
    "    info_day_TEMP = info.loc[info['day_id']==day_id]\n",
    "    locations_day_TEMP = location_cal(info_day_TEMP)\n",
    "    \n",
    "    unit_position_plot_oneday(locations_day_TEMP, day_name, unit_ids = info_day_TEMP['unit_id'].values, with_device_index=False, \n",
    "                              color='gray', s=[400,160], linewidth=2, save_folder=pack_folder)\n",
    "\n",
    "\"\"\"-------------------------\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,20))\n",
    "sw.plot_rasters(we.sorting, time_range=(0, np.sum(day_length)), ax=ax)\n",
    "\n",
    "\"\"\"-------------------------\"\"\"\n",
    "we_long = spikeinterface.extract_waveforms(recording_cmr, merged_sorting, waveform_save_folder+'_long', \n",
    "                                           load_if_exists=we_load_if_exists, ms_before=2, ms_after=2, \n",
    "                                           max_spikes_per_unit=1000000, n_jobs=1, chunk_size=30000)\n",
    "we_long = we_long[0]\n",
    "\n",
    "sorting_unit_show(we_long, recording_cmr, sorting, pack_folder, '_long')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf704f7-ca48-482b-8301-a65ffb44ff21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Waveform Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fface44c-64fa-4c66-937b-2db3ee45707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveform_overlay_plot(info_day_select, unit_ids_select, y_scale_factor=0.8, x_scale_factor=1, \n",
    "                          ylim=[-100,200], y_displace=5, alpha_lim=[0.5,1],save_folder='./'):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------------------\n",
    "    info_day_select: a list of pandas dataframes. Each dataframe is an info pd\n",
    "    unit_ids_select: a list of lists of unit_ids. \n",
    "    y_scale_factor:\n",
    "    x_scale_factor: \n",
    "    ylim:\n",
    "    y_displace:\n",
    "    alpha_lim: think this is used for transparency of template plotting\n",
    "    save_folder:\n",
    "    \n",
    "    Returns\n",
    "    ----------------------------------\n",
    "    FF\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_location = np.array([[160,0],[80,0],[160,80],[80,80],[160,160],\n",
    "                                 [80,160],[160,240],[80,240],[160,320],[80,320],\n",
    "                                 [160,400],[80,400],[160,480],[80,480],[160,560],\n",
    "                                 [80,560],[240,0],[0,0],[240,80],[0,80],\n",
    "                                 [240,160],[0,160],[240,240],[0,240],[240,320],\n",
    "                                 [0,320],[240,400],[0,400],[240,480],[0,480],\n",
    "                                 [240,560],[0,560]])\n",
    "\n",
    "    sensor_channels = np.arange(32)\n",
    "    \n",
    "    cm = pylab.get_cmap('gist_rainbow')\n",
    "    NUM_COLORS = len(unit_ids_select[0])\n",
    "    colors = []\n",
    "    for i in range(NUM_COLORS):\n",
    "        colors.append(cm(1. * i / NUM_COLORS))\n",
    "        \n",
    "    alpha_s = np.arange(alpha_lim[0],alpha_lim[1]+(alpha_lim[1]-alpha_lim[0])/(len(unit_ids_select)-1),\n",
    "                        (alpha_lim[1]-alpha_lim[0])/(len(unit_ids_select)-1))\n",
    "        \n",
    "    if os.path.exists(save_folder)==False:\n",
    "        os.mkdir(save_folder)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(unit_ids_select[0]), 1, figsize=(6,10*len(unit_ids_select[0])))\n",
    "    save_name = 'waveform_similarity_overlay_'\n",
    "    \n",
    "    # Not sure what this does???  \n",
    "    for align_id in range(len(unit_ids_select)):\n",
    "        info_day = info_day_select[align_id] \n",
    "        day_name = info_day['day_name'].values[0]\n",
    "        save_name = save_name + '_' + day_name\n",
    "    \n",
    "    # Outer loop is the unit_id, inner loop is the day (so plot each unit one by one)\n",
    "    for unit_id_id, unit_id in enumerate(unit_ids_select[0]): # So unit_id_id is the count of the unit_id, and unit_id is the actual unit_id\n",
    "        for align_id in range(len(unit_ids_select)): # align_id tracks the day; the length of unit_ids_select is the number of days\n",
    "            \n",
    "            info_day = info_day_select[align_id] # info_day is the info for the given day\n",
    "            unit_ids = unit_ids_select[align_id] # unit_ids are the unit_ids for the given day\n",
    "            day_name = info_day['day_name'].values[0] # this gets the day_name for the first element in info_day. Since all in info_day are same day, this is fine\n",
    "            ax = axes[unit_id_id] # gets a handle on the appropriate subplot axis\n",
    "            plot_channel_ids = sensor_channels # Get all channel ids\n",
    "            \n",
    "            try:\n",
    "                template = info_day.loc[info_day['unit_id']==unit_id]['template'].values[0][:, plot_channel_ids] # for this day and this unit_id, get the templates at the channel_ids at this shank\n",
    "                skip = 0\n",
    "            except:\n",
    "                # template = np.zeros(info_day.iloc[0]['template'][:, plot_channel_ids].shape)\n",
    "                skip = 1\n",
    "            \n",
    "            tps = np.arange(template.shape[0]) # time points\n",
    "            \n",
    "            for idx in range(32):\n",
    "                \"\"\"\n",
    "                x = (tps-template.shape[0]/3) * x_scale_factor + sensor_location[idx][0] # The x-position\n",
    "                y = y_scale_factor * (template[:,idx] + sensor_location[idx][1] - y_displace*align_id) # The y-position\n",
    "                ax.plot(x, y,linewidth=3,c=colors[unit_id_id], alpha=alpha_s[align_id]) # Plot this thing\n",
    "                \"\"\"\n",
    "                if (skip==0):\n",
    "                    x = tps*x_scale_factor + sensor_location[idx][0] # The x-position\n",
    "                    y = y_scale_factor * (template[:,idx] - y_displace*align_id) + sensor_location[idx][1] # The y-position\n",
    "                    ax.plot(x, y,linewidth=3,c=colors[unit_id_id], alpha=alpha_s[align_id]) # Plot this thing\n",
    "        \n",
    "        ax.set_title(f'unit{unit_id}')\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.axis('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.savefig(save_folder+save_name+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ecdd9a-5b2e-4341-9e70-c69990f87a65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Waveform Evolution Per Electrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4c863-9fd1-47ea-9671-86e86818aea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_day_select = []\n",
    "unit_ids_select = []\n",
    "        \n",
    "for day_name in date_id_all:\n",
    "    \n",
    "    info_day = info.loc[info['day_name']==day_name]\n",
    "    unit_ids = curated_ids\n",
    "    \n",
    "    info_day_select.append(info_day)\n",
    "    unit_ids_select.append(unit_ids)\n",
    "\n",
    "waveform_overlay_plot(info_day_select, unit_ids_select, ylim=[-500,200], y_scale_factor=0.35, \n",
    "                      x_scale_factor=1, y_displace=20, alpha_lim=[0.3,1], save_folder=pack_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d043c-b25c-46ee-b155-b1da5e52c42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for unit_id in curated_ids:\n",
    "    print('unit_id:', unit_id)\n",
    "    print(info.loc[info['unit_id']==unit_id]['day_name'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a582e15-f703-4aa6-a589-8c694a692b4e",
   "metadata": {},
   "source": [
    "## Waveform Evolution Extremum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087b307-1d7c-4be7-8f64-6ad69fb31b4b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for day_id, day_name in enumerate(date_id_all):\n",
    "    \n",
    "    data_folder_day = data_folder_all + day_name + '/'\n",
    "    pack_folder_day = pack_folder + day_name + '/'\n",
    "    slice_curated_ids = [np.nan]*NumShanks\n",
    "    shank_ids = []\n",
    "    \n",
    "    waveform_folder_day = pack_folder_day + 'waveforms_merged/'\n",
    "    we_day = WaveformExtractor.load_from_folder(waveform_folder_day)\n",
    "    curated_ids_day = we_day.sorting.unit_ids\n",
    "    template_day = we_day.get_all_templates(unit_ids=curated_ids_day)\n",
    "    \n",
    "    sorting_unit_show(we_day, we_day.recording, we_day.sorting, pack_folder, f'_byday_{day_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbe973-539f-4466-960f-42d3d61704e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Waveform Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d9d28-af6a-4ebe-a968-051f0413246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveform_similarity_cal(choosen_day, template_day0, template_day_follow):\n",
    "    \n",
    "    r_values = []\n",
    "    p_values = []\n",
    "    for idx, choosen_day_id in enumerate(choosen_day):\n",
    "        \n",
    "        template_x = np.reshape(template_day0[choosen_day_id[0]].T,(-1,))\n",
    "        template_y = np.reshape(template_day_follow[choosen_day_id[1]].T,(-1,))\n",
    "        \n",
    "        r_value, p_value = pearsonr(template_x, template_y)\n",
    "        \n",
    "        r_values.append(r_value)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    return np.array(r_values), np.array(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed12b7e-7888-45d8-b1b5-a875138a75ae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "r_values_within_all = []\n",
    "r_values_across_all = []\n",
    "\n",
    "# day0 is the first day of the pair\n",
    "for day0_id, day0_name in enumerate(date_id_all[:-1]):\n",
    "    info_day0 = info.loc[info['day_name']==day0_name]\n",
    "    unit_ids_day0 = info_day0['unit_id'].values\n",
    "    template_day0 = info_day0['template'].values\n",
    "\n",
    "    # day1 is the second day of the pair\n",
    "    for day1_name in date_id_all[(day0_id+1):]:\n",
    "        info_day1 = info.loc[info['day_name']==day1_name]\n",
    "        unit_ids_day1 = info_day1['unit_id'].values\n",
    "        template_day1 = info_day1['template'].values\n",
    "                \n",
    "        print(type(template_day1))\n",
    "        print(template_day1.shape)\n",
    "        \n",
    "        within_choosen = []\n",
    "        across_choosen = []\n",
    "        for unit_day0_count, unit_day0 in enumerate(unit_ids_day0):\n",
    "            for unit_day1_count, unit_day1 in enumerate(unit_ids_day1):\n",
    "                if unit_day0 == unit_day1:\n",
    "                    within_choosen.append((unit_day0_count, unit_day1_count))\n",
    "                else:\n",
    "                    across_choosen.append((unit_day0_count, unit_day1_count))\n",
    "        \n",
    "        print(within_choosen)\n",
    "        print(across_choosen)\n",
    "        r_values_within, _ = waveform_similarity_cal(within_choosen, template_day0, template_day1)\n",
    "        r_values_across, _ = waveform_similarity_cal(across_choosen, template_day0, template_day1)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        maxbin = 1\n",
    "        minbin = -0.5\n",
    "        binsize = 0.02\n",
    "        bin_counts_day_within, bin_edges = np.histogram(r_values_within, \n",
    "                                                        bins=np.arange(minbin,maxbin+binsize,binsize), \n",
    "                                                        density=True)\n",
    "        bin_counts_day_across, bin_edges = np.histogram(r_values_across, \n",
    "                                                        bins=np.arange(minbin,maxbin+binsize,binsize), \n",
    "                                                        density=True)\n",
    "\n",
    "        line1 = ax.bar(bin_edges[:-1]+binsize/2,bin_counts_day_within*binsize,width=binsize,color='red', alpha=0.7)\n",
    "        line2 = ax.bar(bin_edges[:-1]+binsize/2,bin_counts_day_across*binsize,width=binsize,color='gray', alpha=0.7)\n",
    "        ax.legend([line1, line2],['Within units', 'Across units'])\n",
    "        ax.set_title(f'{day0_name}-{day1_name}')\n",
    "        ax.set_xlabel('Waveform similarity')\n",
    "        ax.set_ylabel('Probability')\n",
    "        \n",
    "        r_values_within_all.append(r_values_within)\n",
    "        r_values_across_all.append(r_values_across)\n",
    "\n",
    "r_values_within_all = np.hstack(r_values_within_all)\n",
    "r_values_across_all = np.hstack(r_values_across_all)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "maxbin = 1\n",
    "minbin = -0.5\n",
    "binsize = 0.02\n",
    "bin_counts_day_within, bin_edges = np.histogram(r_values_within_all, bins=np.arange(minbin,maxbin+binsize,binsize), density=True)\n",
    "bin_counts_day_across, bin_edges = np.histogram(r_values_across_all, bins=np.arange(minbin,maxbin+binsize,binsize), density=True)\n",
    "\n",
    "line1 = ax.bar(bin_edges[:-1]+binsize/2,bin_counts_day_within*binsize,width=binsize,color='red', alpha=0.7)\n",
    "line2 = ax.bar(bin_edges[:-1]+binsize/2,bin_counts_day_across*binsize,width=binsize,color='gray', alpha=0.7)\n",
    "ax.legend([line1, line2],['Within units', 'Across units'])\n",
    "ax.set_title('All days')\n",
    "ax.set_xlabel('Waveform similarity')\n",
    "ax.set_ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79fab6-17ea-4f27-9c0f-d28cd953ce50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11cd914-8769-479d-b457-cc04cf981e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction_cal(info_day_select, method='umap',  n_components=2, svd_solver='auto', nb_points = 20000, \n",
    "                                 umap_params={'n_neighbors': 20, 'random_state': 2, 'min_dist': 0.1, 'metric': 'euclidean'}, points_nb=10000):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------\n",
    "    info_day_select:\n",
    "    method:\n",
    "    n_components:\n",
    "    svd_solver:\n",
    "    nb_points:\n",
    "    umap_params:\n",
    "    points_nb:\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_location = np.array([[160,0],[80,0],[160,80],[80,80],[160,160],\n",
    "                                [80,160],[160,240],[80,240],[160,320],[80,320],\n",
    "                                [160,400],[80,400],[160,480],[80,480],[160,560],\n",
    "                                [80,560],[240,0],[0,0],[240,80],[0,80],\n",
    "                                [240,160],[0,160],[240,240],[0,240],[240,320],\n",
    "                                [0,320],[240,400],[0,400],[240,480],[0,480],\n",
    "                                [240,560],[0,560]])\n",
    "    \n",
    "    sensor_channels = np.arange(32)\n",
    "    \n",
    "    waveforms = []\n",
    "    for day_id in range(len(info_day_select)):\n",
    "        info_day = info_day_select[day_id]\n",
    "        for unit_id in range(len(info_day)):\n",
    "            waveform = info_day.iloc[unit_id]['waveform'][:points_nb,:,sensor_channels]\n",
    "            waveforms.append(np.reshape(waveform, (waveform.shape[0], -1), order='F'))\n",
    "            \n",
    "    waveforms = np.vstack(waveforms)\n",
    "    templates = np.mean(waveforms, axis=0)\n",
    "    \n",
    "    waveforms = []\n",
    "    unit_ids = []\n",
    "    day_ids = []\n",
    "    shank_ids = []\n",
    "    for day_id in range(len(info_day_select)):\n",
    "        info_day = info_day_select[day_id]\n",
    "        for unit_id_id, unit_id in enumerate(info_day['unit_id'].values):\n",
    "            \n",
    "            waveform = info_day.iloc[unit_id_id]['waveform'][:,:,sensor_channels]\n",
    "            waveform = np.reshape(waveform, (waveform.shape[0], -1), order='F')\n",
    "            select_indices = np.argsort(np.sqrt(np.sum(np.square(waveform-templates),axis=1)))[:nb_points]\n",
    "            print(select_indices.shape)\n",
    "            waveform = waveform[select_indices,:]\n",
    "            waveforms.append(waveform)\n",
    "            unit_ids.append(unit_id*np.ones((waveform.shape[0],)).astype(int))\n",
    "            day_ids.append(day_id*np.ones((waveform.shape[0],)).astype(int))\n",
    "            \n",
    "    waveforms = np.vstack(waveforms)\n",
    "    unit_ids = np.hstack(unit_ids)\n",
    "    day_ids = np.hstack(day_ids)\n",
    "    \n",
    "    if(method=='pca'):\n",
    "        pca_ = PCA(n_components=n_components, svd_solver=svd_solver).fit(waveforms)\n",
    "        waveforms_2d = pca_.transform(waveforms)\n",
    "    \n",
    "    else:\n",
    "        mapper_ = umap.UMAP(n_neighbors=umap_params['n_neighbors'],\n",
    "                            random_state=umap_params['random_state'], \n",
    "                            min_dist=umap_params['min_dist'], \n",
    "                            n_components=n_components, \n",
    "                            metric=umap_params['metric']).fit(waveforms)\n",
    "        waveforms_2d = mapper_.transform(waveforms)\n",
    "    \n",
    "    data = {'waveform_pc1': waveforms_2d[:,0], 'waveform_pc2': waveforms_2d[:,1], \n",
    "            'day_id': day_ids, 'unit_id': unit_ids}\n",
    "    df = pd.DataFrame(data=data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dimensionality_reduction_stability_plot(df, method='umap', figsize=(30, 20), shank_displace_factor=10, \n",
    "                                            day_displace_factor=20, distance=2, nb_points=20000, \n",
    "                                            azim = 270, elev = 15, dist = 7, colors=None, z_scale=1):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_location = np.array([[160,0],[80,0],[160,80],[80,80],[160,160],\n",
    "                                 [80,160],[160,240],[80,240],[160,320],[80,320],\n",
    "                                 [160,400],[80,400],[160,480],[80,480],[160,560],\n",
    "                                 [80,560],[240,0],[0,0],[240,80],[0,80],\n",
    "                                 [240,160],[0,160],[240,240],[0,240],[240,320],\n",
    "                                 [0,320],[240,400],[0,400],[240,480],[0,480],\n",
    "                                 [240,560],[0,560]])\n",
    "\n",
    "    sensor_channels = np.arange(32)\n",
    "    \n",
    "    # Choose label based on our dimension reduction method\n",
    "    if(method=='pca'):\n",
    "        labels=['PC 1', 'PC 2']\n",
    "    else:\n",
    "        labels=['UMAP 1', 'UMAP 2']\n",
    "   \n",
    "    # Set color scheme\n",
    "    if(colors is None):\n",
    "        cm = pylab.get_cmap('rainbow')\n",
    "        colors = []\n",
    "        NUM_COLORS = len(np.unique(df['unit_id'].values))\n",
    "        for i in range(NUM_COLORS):\n",
    "            colors.append(cm(1. * i / NUM_COLORS))\n",
    "    \n",
    "    day_ids = np.unique(df['day_id'].values) # The unique day_ids\n",
    "    day_displace = np.arange(len(day_ids))*day_displace_factor\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    color_displace = 0\n",
    "    \n",
    "    unit_ids = np.unique(df['unit_id'].values) # The unique unit_ids\n",
    "    centroids_all = {}\n",
    "    for unit_id_id, unit_id in enumerate(unit_ids):\n",
    "        df_unit = df.loc[df['unit_id']==unit_id]\n",
    "        waveform_umap = df_unit[['waveform_pc1','waveform_pc2']].values\n",
    "        centroid = np.mean(waveform_umap,axis=0)\n",
    "        centroids_all[str(unit_id)] = centroid\n",
    "    \n",
    "    centroids_all_day = []\n",
    "    for day_id_id, day_id in enumerate(day_ids):\n",
    "        df_day = df.loc[df['day_id']==day_id] # info for a specific day_id\n",
    "        unit_ids = np.unique(df_day['unit_id'].values) # the unique unit_ids for that day\n",
    "        centroids = {}\n",
    "        for unit_id_id, unit_id in enumerate(unit_ids):\n",
    "            df_unit = df_day.loc[df_day['unit_id']==unit_id]\n",
    "            waveform_umap = df_unit[['waveform_pc1','waveform_pc2']].values\n",
    "            centroid_all = centroids_all[str(unit_id)]\n",
    "            distance_matrix = np.linalg.norm(waveform_umap - centroid_all, axis=1)\n",
    "            ind_points1 = np.random.permutation(len(distance_matrix))#[:2*nb_points]\n",
    "            ind_points = distance_matrix[ind_points1] < distance*2\n",
    "            waveform_umap_ = waveform_umap[ind_points1,:][ind_points,:]\n",
    "            centroid = np.mean(waveform_umap_,axis=0)\n",
    "\n",
    "            distance_matrix = np.linalg.norm(waveform_umap_ - centroid, axis=1)\n",
    "            #keep closest points to centroid\n",
    "            ind_points1 = np.random.permutation(len(distance_matrix))#[:nb_points]\n",
    "            ind_points = distance_matrix[ind_points1] < distance\n",
    "            waveform_umap_ = waveform_umap_[ind_points1,:][ind_points,:]\n",
    "\n",
    "            centroid_new = np.mean(waveform_umap_,axis=0)\n",
    "            centroids[str(unit_id)] = centroid_new\n",
    "            color_plot = colors[unit_id_id]\n",
    "\n",
    "            ax.scatter(waveform_umap_[:,1], waveform_umap_[:,0], \n",
    "                       day_displace[day_id_id]*np.ones((waveform_umap_.shape[0],))+0.1,\n",
    "                       edgecolor= 'grey', s=10, linewidth=0.1,alpha=0.5,\n",
    "                       color=color_plot, zorder=-1)\n",
    "        \n",
    "        centroids_all_day.append(centroids)\n",
    "    \n",
    "    for day_id_id, day_id in enumerate(day_ids):\n",
    "        \n",
    "        if day_id_id == 0:\n",
    "            continue\n",
    "        \n",
    "        df_day_curr = df.loc[df['day_id']==day_id] # info for a specific day_id\n",
    "        \n",
    "        # These are the unit_ids for the current day\n",
    "        centroids_unit_curr = np.unique(df_day_curr['unit_id'].values)\n",
    "\n",
    "        # Right now these are all dictionaries where the key is the unit_id of the current day, the value is the centroid of the current day\n",
    "        centroids_plot_curr = centroids_all_day[day_id_id].copy()\n",
    "        centroids_plot_past = centroids_all_day[day_id_id].copy()\n",
    "        \n",
    "        centroids_daycount = centroids_all_day[day_id_id].copy()\n",
    "        centroids_daycount = dict.fromkeys(centroids_daycount.keys(), day_id_id)\n",
    "        \n",
    "        print(centroids_daycount)\n",
    "        \n",
    "        # print(list(centroids_plot_curr.values())[0][0])\n",
    "        \n",
    "        # Now we look for the last time these unit_ids occurred, and their day and centroid\n",
    "        for unit_id_id, unit_id in enumerate(centroids_unit_curr):\n",
    "            for day_id_id_past, day_id_past in enumerate(day_ids[:day_id_id]):\n",
    "                if unit_id in df.loc[df['day_id']==day_id_past]['unit_id'].values:\n",
    "                    centroids_plot_past[str(unit_id)] = centroids_all_day[day_id_id_past][str(unit_id)]\n",
    "                    centroids_daycount[str(unit_id)] = day_id_id_past\n",
    "\n",
    "        centroids_plot_curr = list(centroids_plot_curr.values())\n",
    "        centroids_plot_past = list(centroids_plot_past.values())\n",
    "        centroids_daycount = list(centroids_daycount.values())\n",
    "        \n",
    "        print(centroids_plot_curr)\n",
    "        print(centroids_plot_past)\n",
    "        print(centroids_daycount)\n",
    "        \n",
    "        for j in range(len(centroids_plot_curr)):\n",
    "            color = colors[int(j+color_displace)]\n",
    "            ax.plot([centroids_plot_past[j][1], centroids_plot_curr[j][1]],\n",
    "                    [centroids_plot_past[j][0], centroids_plot_curr[j][0]],\n",
    "                    [day_displace[centroids_daycount[j]]+1, day_displace[day_id_id]+1],\n",
    "                    color=color,\n",
    "                    marker='o',\n",
    "                    markersize=5,\n",
    "                    markerfacecolor='None',\n",
    "                    markeredgewidth=1,\n",
    "                    markeredgecolor='black',\n",
    "                    alpha=1.,\n",
    "                    linewidth=3,\n",
    "                    zorder=1000)\n",
    "    \n",
    "    color_displace += len(unit_ids)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    x_label = 'umap2'\n",
    "    y_label = 'umap1'\n",
    "    z_label = 'Mouse age (months)'\n",
    "    x_scale, y_scale = 1, 1\n",
    "    \"\"\"Adjust these lines for axis angle\"\"\"\n",
    "    # ax = set_ax_style(ax, x_label, y_label, z_label, x_scale, y_scale, z_scale)\n",
    "    ax.azim = azim\n",
    "    ax.elev = elev\n",
    "    ax.dist = dist\n",
    "   \n",
    "    return ax\n",
    "\n",
    "def set_ax_style(ax, x_label, y_label, z_label,\n",
    "                x_scale, y_scale, z_scale, background=False):\n",
    "    ax.set_xlabel(x_label, labelpad=20)\n",
    "    ax.set_ylabel(y_label, labelpad=20)\n",
    "    ax.set_zlabel(z_label, labelpad=20)\n",
    "    if not background:\n",
    "        #set background to white\n",
    "        ax.xaxis.pane.set_edgecolor('w')\n",
    "        ax.yaxis.pane.set_edgecolor('w')\n",
    "        ax.zaxis.pane.set_edgecolor('w')\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.axis('off')\n",
    "        #ax.yaxis('off')\n",
    "        #ax.zaxis('off')\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "    #scale\n",
    "    scale=np.diag([x_scale, y_scale, z_scale, 1.0])\n",
    "    scale=scale*(1.0/scale.max())\n",
    "    scale[3,3]=1.0 \n",
    "    def short_proj():\n",
    "        return np.dot(Axes3D.get_proj(ax), scale)\n",
    "    ax.get_proj=short_proj\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e862e2-b9fe-40c7-8839-d571a507b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dimensionality_reduction_cal(info_day_select, method='umap',  n_components=2, \n",
    "                                  umap_params={'n_neighbors': 20, 'random_state': 2, 'min_dist': 0.1, 'metric': 'euclidean'})\n",
    "\n",
    "df.to_csv(pack_folder+'dimensionality_reduction_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9ea72-81bf-4568-a3a3-91267bf85fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(pack_folder+'dimensionality_reduction_info.csv')\n",
    "\n",
    "cm = pylab.get_cmap('gist_rainbow')\n",
    "NUM_COLORS = max(np.unique(df['unit_id'].values)) + 1\n",
    "colors = []\n",
    "for i in range(NUM_COLORS):\n",
    "    colors.append(cm(1. * i / NUM_COLORS))\n",
    "colors = np.array(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3516c4-deaf-4993-b613-c83fa89c7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_plot = colors[np.unique(df['unit_id'].values)]\n",
    "ax = dimensionality_reduction_stability_plot(df, method='umap', figsize=(20, 20), shank_displace_factor=10, \n",
    "                                             day_displace_factor=40, distance=1, nb_points=20000, \n",
    "                                             azim = 270, elev = 15, dist = 7, colors=colors_plot, z_scale=2)\n",
    "plt.savefig(pack_folder + f'umap_across_days.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c67d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors_plot = colors[np.unique(df['unit_id'].values)]\n",
    "ax = dimensionality_reduction_stability_plot(df, method='umap', figsize=(20, 20), shank_displace_factor=10, \n",
    "                                             day_displace_factor=40, distance=np.inf, nb_points=np.exp(100), \n",
    "                                             azim = 270, elev = 15, dist = 7, colors=colors_plot, z_scale=2)\n",
    "plt.savefig(pack_folder + f'umap_across_days_not_confined.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed08a2c-4f42-47a6-9135-1edca886f486",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Waveform Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02269ed-f761-48d3-bd0b-44ad162cee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cal(slice_curated_we, quality_scores_cal=True, points_nb=None, \n",
    "                umap_params = {'n_neighbors': 20, 'random_state': 2, 'min_dist': 0.1, 'metric': 'euclidean', 'n_components': 2}):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    probe_groups = np.arange(0,32)\n",
    "    qc_metric_names=['snr', 'firing_rate']\n",
    "    we_feature_names=['peak_to_valley','peak_trough_ratio', 'halfwidth', 'repolarization_slope','recovery_slope']\n",
    "    pc = st.compute_principal_components(slice_curated_we, load_if_exists=False, n_components=2, mode='by_channel_local')\n",
    "    qc_metrics = st.compute_quality_metrics(slice_curated_we, metric_names=qc_metric_names)\n",
    "    unit_ids = slice_curated_we.sorting.unit_ids\n",
    "    extremum_channel_ids = st.get_template_extremum_channel(slice_curated_we, peak_sign='neg')\n",
    "    sampling_frequency = slice_curated_we.sorting.get_sampling_frequency()\n",
    "    \n",
    "    templates = []\n",
    "    amplitudes = []\n",
    "    waveforms = []\n",
    "    waveforms_all_channel = []\n",
    "    waveform_labels = []\n",
    "    shank_ids = []\n",
    "    for unit_id in unit_ids:\n",
    "        extremum_channel_id = extremum_channel_ids[unit_id]\n",
    "        shank_id = probe_groups[extremum_channel_id]\n",
    "        waveform = slice_curated_we.get_waveforms(unit_id=unit_id)\n",
    "        template = slice_curated_we.get_template(unit_id=unit_id)\n",
    "        amplitude = np.max(template,axis=0) - np.min(template,axis=0)\n",
    "        select_channels = np.where(probe_groups==shank_id)[0]\n",
    "        amplitude = amplitude[extremum_channel_id]\n",
    "        waveform_shank = waveform[:,:,select_channels]\n",
    "        waveform_mean = np.mean(waveform_shank,axis=0)\n",
    "\n",
    "        if(points_nb is not None):\n",
    "            select_indices = np.argsort(np.sum(np.sum(np.square((waveform_shank - waveform_mean)),\n",
    "                                                      axis=1),axis=1))[:points_nb]\n",
    "            waveform = waveform[select_indices,:,:]\n",
    "        \n",
    "        waveform_all_channel = np.reshape(waveform[:,:,select_channels], (waveform.shape[0],-1), order='F')\n",
    "        waveform = waveform[:,:,extremum_channel_id]\n",
    "        templates.append(template)\n",
    "        amplitudes.append(amplitude)\n",
    "        waveforms.append(waveform)\n",
    "        waveforms_all_channel.append(waveform_all_channel)\n",
    "        waveform_labels.append(np.ones((waveform.shape[0],))*unit_id)\n",
    "        shank_ids.append(np.ones((waveform.shape[0],))*shank_id)\n",
    "\n",
    "    templates = np.vstack(templates)\n",
    "    waveforms = np.vstack(waveforms)\n",
    "    waveforms_all_channel = np.vstack(waveforms_all_channel)\n",
    "    waveform_labels = np.hstack(waveform_labels)\n",
    "    shank_ids = np.hstack(shank_ids)\n",
    "    \n",
    "    we_metrics = features_5(waveforms, sampling_frequency, feature_names=we_feature_names)\n",
    "    \n",
    "    metrics = {}\n",
    "    for we_feature_name in we_feature_names:\n",
    "        feature_neurons = []\n",
    "        feature = we_metrics[we_feature_name]\n",
    "        for unit_id in unit_ids:\n",
    "            feature_neuron = feature[waveform_labels==unit_id]\n",
    "            feature_neuron_good = feature_neuron[~np.isnan(feature_neuron)]\n",
    "            feature_neuron_good_mean  = np.mean(feature_neuron_good)\n",
    "            feature_neuron_good_std = np.std(feature_neuron_good)\n",
    "            right_indices = np.where(((feature_neuron_good<feature_neuron_good_mean+3*feature_neuron_good_std)*1 + \n",
    "                                     (feature_neuron_good>feature_neuron_good_mean-3*feature_neuron_good_std)*1)>1)[0]\n",
    "            feature_neuron_good = np.mean(feature_neuron_good[right_indices])\n",
    "            feature_neurons.append(feature_neuron_good)\n",
    "        \n",
    "        metrics[we_feature_name] = feature_neurons\n",
    "        \n",
    "    metrics['amplitude'] = np.array(amplitudes)\n",
    "    metrics['snr'] = qc_metrics['snr'].values\n",
    "    metrics['firing_rate'] = qc_metrics['firing_rate'].values\n",
    "    \n",
    "    if(quality_scores_cal==True):\n",
    "        metrics['l_ratio'] = np.empty(unit_ids.shape)*np.nan\n",
    "        metrics['sil_scores'] = np.empty(unit_ids.shape)*np.nan\n",
    "        for shank_id in np.unique(shank_ids):\n",
    "            idx = np.where(shank_ids==shank_id)[0]\n",
    "            waveform_labels_shank = waveform_labels[idx]\n",
    "            unit_ids_shank = np.unique(waveform_labels_shank)\n",
    "            if(len(unit_ids_shank)>1):\n",
    "#                 waveforms_shank = waveforms_all_channel[idx,:]\n",
    "                unit_smallest_spike_num = np.min(np.array([np.sum(waveform_labels_shank==unit_id) \n",
    "                                                           for unit_id in unit_ids_shank]))\n",
    "                waveforms_shank = np.vstack([waveforms_all_channel[waveform_labels==unit_id][:unit_smallest_spike_num]\n",
    "                                            for unit_id in unit_ids_shank])\n",
    "                waveform_labels_shank = np.hstack([waveform_labels[waveform_labels==unit_id][:unit_smallest_spike_num]\n",
    "                                            for unit_id in unit_ids_shank])\n",
    "\n",
    "\n",
    "                mapper_ = umap.UMAP(n_neighbors=umap_params['n_neighbors'],\n",
    "                                            random_state=umap_params['random_state'], \n",
    "                                            min_dist=umap_params['min_dist'],\n",
    "                                            n_components=umap_params['n_components'],\n",
    "                                            metric=umap_params['metric']).fit(waveforms_shank)\n",
    "                shank_waveforms_2d = mapper_.transform(waveforms_shank)\n",
    "\n",
    "                sil_scores = silhouette_score(shank_waveforms_2d, waveform_labels_shank)\n",
    "\n",
    "                for unit_id in unit_ids_shank:\n",
    "                    pcs_for_this_unit = shank_waveforms_2d[waveform_labels_shank == unit_id,:]\n",
    "                    pcs_for_other_units = shank_waveforms_2d[waveform_labels_shank != unit_id, :]\n",
    "                    mean_value = np.expand_dims(np.mean(pcs_for_this_unit,0),0)\n",
    "                    VI = np.linalg.inv(np.cov(pcs_for_this_unit.T))\\\n",
    "                    \n",
    "                    mahalanobis_other = np.sort(cdist(mean_value, pcs_for_other_units, 'mahalanobis', VI = VI)[0])\n",
    "                    mahalanobis_self = np.sort(cdist(mean_value, pcs_for_this_unit, 'mahalanobis', VI = VI)[0])\n",
    "\n",
    "                    n = np.min([pcs_for_this_unit.shape[0], pcs_for_other_units.shape[0]]) # number of spikes\n",
    "                    if n >= 2:\n",
    "                        dof = pcs_for_this_unit.shape[1] # number of features\n",
    "                        l_ratio = np.sum(1 - chi2.cdf(pow(mahalanobis_other,2), dof)) / \\\n",
    "                                mahalanobis_self.shape[0] # normalize by size of cluster, not number of other spikes\n",
    "                    else:\n",
    "                        l_ratio = np.nan\n",
    "\n",
    "                    idx_unit = np.where(unit_ids==unit_id)[0]\n",
    "                    metrics['l_ratio'][idx_unit] = l_ratio\n",
    "\n",
    "                _, idx_unit = ismember(unit_ids_shank, unit_ids)\n",
    "                metrics['sil_scores'][idx_unit] = np.ones((len(unit_ids_shank),))*sil_scores\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def feature_dist_cal(slice_curated_we, quality_scores_cal=True, points_nb=None, \n",
    "                     umap_params = {'n_neighbors': 20, 'random_state': 2, 'min_dist': 0.1, 'metric': 'euclidean', 'n_components': 2}, \n",
    "                     save_folder='./'):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    \n",
    "    probe_groups = np.arange(0,32)\n",
    "    we_feature_names=['peak_to_valley','peak_trough_ratio', 'halfwidth', 'repolarization_slope','recovery_slope']\n",
    "    unit_ids = slice_curated_we.sorting.unit_ids\n",
    "    extremum_channel_ids = st.get_template_extremum_channel(slice_curated_we, peak_sign='neg')\n",
    "    sampling_frequency = slice_curated_we.sorting.get_sampling_frequency()\n",
    "    \n",
    "    templates = []\n",
    "    amplitudes = []\n",
    "    waveforms = []\n",
    "    waveforms_all_channel = []\n",
    "    waveform_labels = []\n",
    "    shank_ids = []\n",
    "    \n",
    "    for unit_id in unit_ids:\n",
    "        extremum_channel_id = extremum_channel_ids[unit_id]\n",
    "        shank_id = probe_groups[extremum_channel_id]\n",
    "        waveform = slice_curated_we.get_waveforms(unit_id=unit_id)\n",
    "        template = slice_curated_we.get_template(unit_id=unit_id)\n",
    "        amplitude = np.max(template,axis=0) - np.min(template,axis=0)\n",
    "        select_channels = np.where(probe_groups==shank_id)[0]\n",
    "        amplitude = amplitude[extremum_channel_id]\n",
    "        waveform_shank = waveform[:,:,select_channels]\n",
    "        waveform_mean = np.mean(waveform_shank,axis=0)\n",
    "\n",
    "        if(points_nb is not None):\n",
    "            select_indices = np.argsort(np.sum(np.sum(np.square((waveform_shank - waveform_mean)),\n",
    "                                                      axis=1),axis=1))[:points_nb]\n",
    "            waveform = waveform[select_indices,:,:]\n",
    "        \n",
    "        waveform_all_channel = np.reshape(waveform[:,:,select_channels], (waveform.shape[0],-1), order='F')\n",
    "        waveform = waveform[:,:,extremum_channel_id]\n",
    "        templates.append(template)\n",
    "        amplitudes.append(amplitude)\n",
    "        waveforms.append(waveform)\n",
    "        waveforms_all_channel.append(waveform_all_channel)\n",
    "        waveform_labels.append(np.ones((waveform.shape[0],))*unit_id)\n",
    "        shank_ids.append(np.ones((waveform.shape[0],))*shank_id)\n",
    "\n",
    "    templates = np.vstack(templates)\n",
    "    waveforms = np.vstack(waveforms)\n",
    "    waveforms_all_channel = np.vstack(waveforms_all_channel)\n",
    "    waveform_labels = np.hstack(waveform_labels)\n",
    "    shank_ids = np.hstack(shank_ids)\n",
    "    \n",
    "    we_metrics = features_5(waveforms, sampling_frequency, feature_names=we_feature_names)\n",
    "    \n",
    "    for we_feature_name in we_feature_names:\n",
    "        feature_neurons = []\n",
    "        feature = we_metrics[we_feature_name]\n",
    "        \n",
    "        fig, axs = plt.subplots(int(np.ceil(len(sorting.unit_ids)/4)), 4, figsize=(20, 5*np.ceil(len(sorting.unit_ids)/4)))\n",
    "        \n",
    "        for i, unit_id in enumerate(unit_ids):\n",
    "            feature_neuron = feature[waveform_labels==unit_id]\n",
    "            feature_neuron_good = feature_neuron[~np.isnan(feature_neuron)]\n",
    "            feature_neuron_good_mean  = np.mean(feature_neuron_good)\n",
    "            feature_neuron_good_std = np.std(feature_neuron_good)\n",
    "            right_indices = np.where(((feature_neuron_good<feature_neuron_good_mean+3*feature_neuron_good_std)*1 + \n",
    "                                     (feature_neuron_good>feature_neuron_good_mean-3*feature_neuron_good_std)*1)>1)[0]\n",
    "            feature_neuron_good = feature_neuron_good[right_indices]\n",
    "            \n",
    "            if int(np.ceil(len(unit_ids)/4)) > 1:\n",
    "                ax = axs[int(np.floor(i/4)), int(np.mod(i,4))]\n",
    "                ax.hist(feature_neuron_good, bins=20)\n",
    "                ax.set_title(f'{we_feature_name} - Unit {unit_id}')\n",
    "            else:\n",
    "                ax = axs[int(np.mod(i,4))]\n",
    "                ax.hist(feature_neuron_good, bins=20)\n",
    "                ax.set_title(f'{we_feature_name} - Unit {unit_id}')\n",
    "        \n",
    "        plt.savefig(save_folder+'/waveform_metric_' + we_feature_name + '.pdf',dpi=600)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fcbf8-dc64-4681-bd43-2a07e2065a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_all = ['snr', 'firing_rate', 'amplitude', 'peak_to_valley','peak_trough_ratio', 'halfwidth', 'repolarization_slope','recovery_slope', 'sil_scores', 'l_ratio']\n",
    "quality_scores_cal = False\n",
    "info_load_if_exists = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0468143-e331-4eea-ab41-078181a06e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate waveform features for all-days\"\"\"\n",
    "\n",
    "if(quality_scores_cal==True):\n",
    "    feature_names = feature_names_all\n",
    "else:\n",
    "    feature_names = feature_names_all[:-2]\n",
    "\n",
    "# Calculate the 'average' metrics for units (all-days)\n",
    "umap_params = {'n_neighbors': 10, 'random_state': 2, 'min_dist': 0.05, 'metric': 'euclidean', 'n_components': 10}\n",
    "metrics = feature_cal(we, quality_scores_cal=quality_scores_cal, points_nb=5000, umap_params=umap_params)\n",
    "\n",
    "# Create histograms of select metrics for units (all-days)\n",
    "feature_dist_cal(we, quality_scores_cal=quality_scores_cal, points_nb=5000, umap_params=umap_params, save_folder=pack_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67b8b7-be7d-41db-957e-4f83bfecbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate waveform features for individual days\"\"\"\n",
    "\n",
    "metrics_coll = []\n",
    "\n",
    "# Calculate the 'average' metrics for units (individual days)\n",
    "umap_params = {'n_neighbors': 10, 'random_state': 2, 'min_dist': 0.05, 'metric': 'euclidean', 'n_components': 10}\n",
    "for day_id, day_name in enumerate(date_id_all):\n",
    "    \n",
    "    # Get waveform object for the day\n",
    "    data_folder_day = data_folder_all + day_name + '/'\n",
    "    pack_folder_day = pack_folder + day_name + '/'\n",
    "    waveform_folder_day = pack_folder_day + 'waveforms_merged/'\n",
    "    we_day = WaveformExtractor.load_from_folder(waveform_folder_day)\n",
    "    \n",
    "    # Calculate 'average' metrics for the units in this day\n",
    "    metrics_day = feature_cal(we_day, quality_scores_cal=quality_scores_cal, points_nb=5000, umap_params=umap_params)\n",
    "    metrics_coll.append(metrics_day)\n",
    "    \n",
    "    # Create distribution of metrics for units in this day \n",
    "    feature_dist_cal(we_day, quality_scores_cal=quality_scores_cal, points_nb=5000, umap_params=umap_params, save_folder=pack_folder_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d143f6-1ba5-42fd-955e-dd47a555e7fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed16b33-07e8-4750-b271-7c33cd93fc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace_cmr = recording_cmr.get_traces().T\n",
    "sample_rate = recording_cmr.get_sampling_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf77645-1df1-4122-b035-976cfae4604c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Computes power spectrum using Welch's method to reduce noise\"\"\"\n",
    "\n",
    "fig, axs = plt.subplots(8, 4, figsize=(20, 40))\n",
    "\n",
    "for i in range(trace_cmr.shape[0]):    \n",
    "    ax = axs[i//4, i%4] # iterate through subplots\n",
    "    \n",
    "    # Compute the power spectrum using Welch's method\n",
    "    f, Pxx_spec = signal.welch(trace_cmr[i], sample_rate, 'flattop', 1024, scaling='spectrum')\n",
    "    \n",
    "    ax.semilogy(f, np.sqrt(Pxx_spec))\n",
    "    ax.set_title(f'Shank {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5343017-e2ce-4020-891e-c96db779f00f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Computes power spectrum using Welch's method to reduce noise (individual days)\"\"\"\n",
    "\n",
    "for day_id in range(len(date_id_all)):\n",
    "    \n",
    "    data_folder_day = data_folder_all + date_id_all[day_id] + '/'\n",
    "    pack_folder_day = pack_folder + date_id_all[day_id] + '/'\n",
    "    \n",
    "    # Load recording object\n",
    "    recording_save_path_day = data_folder_day + 'recordings/'\n",
    "    recording_day = spikeinterface.core.base.BaseExtractor.load_from_folder(recording_save_path_day)\n",
    "    recording_f_day = st.preprocessing.bandpass_filter(recording_day, freq_min=freq_min, freq_max=freq_max)\n",
    "    recording_cmr_day = st.preprocessing.common_reference(recording_f_day, reference='global', operator='average')\n",
    "    \n",
    "    # Plot spectograms\n",
    "    trace_cmr_day = recording_cmr_day.get_traces().T\n",
    "    sample_rate_day = recording_cmr_day.get_sampling_frequency()\n",
    "    \n",
    "    fig, axs = plt.subplots(8, 4, figsize=(20, 40))\n",
    "\n",
    "    for i in range(trace_cmr_day.shape[0]):    \n",
    "        ax = axs[i//4, i%4] # iterate through subplots\n",
    "\n",
    "        # Compute the power spectrum using Welch's method\n",
    "        f, Pxx_spec = signal.welch(trace_cmr_day[i], sample_rate_day, 'flattop', 1024, scaling='spectrum')\n",
    "\n",
    "        ax.plot(f, np.sqrt(Pxx_spec))\n",
    "        ax.set_title(f'Shank {i+1}')\n",
    "    \n",
    "    plt.savefig(pack_folder_day+'/power_spectrum.pdf',dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
