{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This pipeline is based on Spikeinterface version > 0.100 to sort Intan data.\n",
    "\n",
    "\"recording\", \"sorting\", and \"analyzer\" are three key components of Spikeinterface processing.\n",
    "1. \"recording\" is recording data stored in Spikeinterface format.\n",
    "2. \"sorting\" is sorted results of Spikeinterface.\n",
    "3. \"analyzer\" is generated from \"sorting\" and \"recording\" to use the post-sorting analyzing packages in Spikeinterface version > 0.100.\n",
    "multi-session processing data and results in this pipeline are defined as \"recordings\" and 'sortings'.\n",
    "\n",
    "\"analyzer\" is used in NEW version of Spikeinterface to post-process the sorted results.\n",
    "\"waveform_extractors\" is used in OLD version of Spikeinterface to post-process the sorted results.\n",
    "You are recommended to do post-processing, curation, and plot with following pipelines developed with \"analyzer\".\n",
    "However, if you need plots related with \"waveform_extractors\" of old version Spikeinterface. \n",
    "For example, you want to check the plot of averaged waveforms during curation, or you want the template on all electrode as final results.\n",
    "Please jump the compatible code with the kernel that has old version Spikeinterface here.\n",
    "\n",
    "IMPORTANT! YOU HAVE TO CHANGE PARAMETERS IN \"Set I/O path and sorter threshold\" AND \"Define the 'probe'\" SECTIONS TO ADAPT NEW DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spikeinterface as si\n",
    "import spikeinterface.core as sc \n",
    "import spikeinterface.curation as scu\n",
    "import spikeinterface.extractors as se \n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.widgets as sw\n",
    "import sys \n",
    "import quantities as pq\n",
    "import neo\n",
    "import umap\n",
    "import json\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.io import savemat\n",
    "from probeinterface import generate_multi_columns_probe\n",
    "from probeinterface.plotting import plot_probe\n",
    "from probeinterface.utils import combine_probes\n",
    "from elephant.gpfa import GPFA\n",
    "from sklearn.decomposition import PCA\n",
    "from src.importrhdutilities import load_file\n",
    "\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set I/O path and sorter threshold\n",
    "Have to change to adapt new data: project_name, subject_name, processing_name, segment_paths, output_root, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This cells contains the parameters that has to change to adapt new data\n",
    "\"\"\"\n",
    "project_name = 'axolotl_tail_amputation_multisession_64ch' # The parent folder name of recording data, recommended to be set as the name of project\n",
    "subject_name = 'axolotl_A' # The folder name of recording data, recommended to be set as the name of subject for each experiment\n",
    "processing_name = '101424-101824-7sections' # The folder name of individual processing, recommended to be set as the combination name of all recording segments input\n",
    "segment_paths = [f'data/{project_name}//{subject_name}/axolotl_embryo_before_amputation_1',\n",
    "                f'data/{project_name}//{subject_name}/axolotl_embryo_before_amputation_2',\n",
    "                f'data/{project_name}//{subject_name}/axolotl_embryo_after_amputation_3',\n",
    "                f'data/{project_name}//{subject_name}/axolotl_embryo_after_amputation_4',\n",
    "                f'data/{project_name}//{subject_name}/axolotl_embryo_after_amputation_5',\n",
    "                f'data/{project_name}//{subject_name}/axolotl_embryo_after_amputation_6',\n",
    "                f'data/{project_name}//{subject_name}/axolotl_embryo_after_amputation_7',\n",
    "                ] # The folder path of all recording segments need to be concatenated\n",
    "\n",
    "\"\"\"\n",
    "# Adjust threshold(sorter_threshold) for sorting as needed. The \"sorting\" and analyzer\" are named with suffix of threshold\n",
    "\"\"\"\n",
    "sorter_threshold = 3.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Dont' change this cell\n",
    "\"\"\"\n",
    "output_root = f'data/processed/{project_name}/{subject_name}/{processing_name}'# The output root of processed results\n",
    "\n",
    "recordings_folder = f'{output_root}/recordings' # Define the path where the recordings will be saved\n",
    "sortings_folder = f'{output_root}/sortings-{sorter_threshold}' # Define the path where the sortings will be saved\n",
    "analyzer_folder = f'{output_root}/analyzer-{sorter_threshold}' # Define the path where the \"analyzer\" will be saved\n",
    "analyzers_folder = f'{output_root}/analyzers-{sorter_threshold}' # Define the path where the \"analyzer\" will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the \"probe\"\n",
    "Have to change to adapt new data:   intan_channel_indices = np.array(), \n",
    "                                    probe0 = generate_multi_columns_probe(), \n",
    "                                    probe0.rotate(), \n",
    "                                    probe0.set_device_channel_indices()\n",
    "                            \n",
    "Add more probes as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Intan data's all channel indices\n",
    "# change based on the channel number that Intan data has signal and need to be sorted\n",
    "\n",
    "\n",
    "# If you use multiple Intan chips, their channel indices is continuously listed.\n",
    "# For example, you use 64ch Intan chip at port A and 64 Intan chip at port B, then recorded Data will be ch0-63 for chip A and ch64-127 for chip B\n",
    "# You use 16-47ch of 64ch Intan chip at port A and 16-47ch of 64ch Intan chip at port B, then recorded Data will be ch0-31 for chip A and ch32-63 for chip B\n",
    "\"\"\"\n",
    "# 64 ch channel indices\n",
    "intan_channel_indices = np.array([ \n",
    "    [0, 1, 2, 3, 4, 5, 6, 7], \n",
    "    [8, 9, 10, 11, 12, 13, 14, 15], \n",
    "    [16, 17, 18, 19, 20, 21, 22, 23], \n",
    "    [24, 25, 26, 27, 28, 29, 30, 31],\n",
    "    [32, 33, 34, 35, 36, 37, 38, 39], \n",
    "    [40, 41, 42, 43, 44, 45, 46, 47], \n",
    "    [48, 49, 50, 51, 52, 53, 54, 55], \n",
    "    [56, 57, 58, 59, 60, 61, 62, 63],\n",
    "]) \n",
    "\n",
    "def find_shank(channel_index):\n",
    "    for shank, shank_indices in enumerate(intan_channel_indices):\n",
    "        if channel_index in shank_indices:\n",
    "            return shank\n",
    "\n",
    "\n",
    "def create_probe(channel_indices, savepath=None, show_probe=True):\n",
    "    n_shank = len(channel_indices)\n",
    "    n_channel = channel_indices.size\n",
    "    shank_locations = np.array([[0, 0]])\n",
    "\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    \"\"\"\n",
    "    # Probe layout\n",
    "    # change based on the geometry design of probe in the mask\n",
    "    # not necessary to be probe0, could be multi-probe (probe0, probe1..) as convenience, just need to make sure the final geometry looks right. \n",
    "    \"\"\"\n",
    "    probe0 = generate_multi_columns_probe(num_columns=8,\n",
    "                                        num_contact_per_column=8,\n",
    "                                        xpitch=50, ypitch=50,\n",
    "                                        contact_shapes='circle', contact_shape_params={'radius': 8},\n",
    "                                        )\n",
    "    probe0.rotate(0)\n",
    "    \n",
    "    \"\"\"\n",
    "    # probe(device)'s channel indices \n",
    "    # change based the how Intan data channel indices match to probe(device) channel indices, need to check the intan headstage electrode connector pinout, PCA layout and mask design \n",
    "    \"\"\"\n",
    "    probe0.set_device_channel_indices([31, 29, 27, 25, 23, 21, 19, 17, \n",
    "    15, 13, 11, 9, 7, 5, 3, 1, \n",
    "    0, 2, 4, 6, 8, 10, 12, 14, \n",
    "    16, 18, 20, 22, 24, 26, 28, 30,\n",
    "    49, 51, 53, 55, 57, 59, 61, 63, \n",
    "    33, 35, 37, 39, 41, 43, 45, 47, \n",
    "    46, 44, 42, 40, 38, 36, 34, 32, \n",
    "    62, 60, 58, 56, 54, 52, 50, 58,\n",
    "    ])\n",
    "    plot_probe(probe0, with_device_index=True, ax=ax)\n",
    "\n",
    "    multi_shank_probe = combine_probes([probe0])\n",
    "    multi_shank_probe.set_device_channel_indices(channel_indices.flatten())\n",
    "\n",
    "    n_channel = 64\n",
    "    n_shank = 1\n",
    "\n",
    "    plt.xlim(-50, 400)\n",
    "    plt.ylim(-150, 400)\n",
    "    plt.title(f'Probe - {n_channel}ch - {n_shank}shanks')\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath, bbox_inches='tight')\n",
    "        \n",
    "    if show_probe:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return multi_shank_probe\n",
    "\n",
    "probe = create_probe(intan_channel_indices, show_probe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Intan data\n",
    "Optional to change to adapt new data to choose channel: if file_traces.shape[0] == 128, selected_channels = list(range()) + list(range())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The code in this cell has a function to delete the redundant Intan channels\n",
    "# For example, in the first recording, you use 64 channel Intan chip to record 32 channels, \n",
    "# in the second recording, you forget to turn off the redundant channels\n",
    "# The code here delete the redundant channels to make sure the data and concatenated together correctly\n",
    "\n",
    "# Use list(range()) + list(range()) to choose the channel indices that has real signal\n",
    "\"\"\"\n",
    "\n",
    "# Reading data in selected channels\n",
    "session_info_file = f'{output_root}/session_info.csv'\n",
    "if not os.path.isfile(session_info_file):\n",
    "    session_info = []\n",
    "    file_start = 0\n",
    "    os.makedirs(recordings_folder, exist_ok=True)\n",
    "    for segment_index, segment_path in enumerate(segment_paths):\n",
    "        file_index = 0\n",
    "        traces = []\n",
    "        for recording_path in sorted(glob.glob(f'{segment_path}/*.rhd')):\n",
    "            try:\n",
    "                raw_data, data_present = load_file(recording_path)\n",
    "            except:\n",
    "                data_present = False\n",
    "            if data_present:\n",
    "                sampling_frequency = raw_data['frequency_parameters']['amplifier_sample_rate']\n",
    "                file_traces = raw_data['amplifier_data']\n",
    "                \n",
    "                # Check if the data has 128 channels\n",
    "                if file_traces.shape[0] == 128:\n",
    "                    # Select channels 16-47 and 80-111\n",
    "                    selected_channels = list(range(16, 48)) + list(range(80, 112))\n",
    "                    file_traces = file_traces[selected_channels, :]\n",
    "                \n",
    "                session_info.append({\n",
    "                    'segment': segment_index,\n",
    "                    'segment_path': segment_path,\n",
    "                    'file_index': file_index,\n",
    "                    'file_path': recording_path,\n",
    "                    'file_start': file_start,\n",
    "                    'file_duration': file_traces.shape[1],\n",
    "                })\n",
    "                traces.append(file_traces)   \n",
    "                file_start += file_traces.shape[1]  \n",
    "                file_index += 1\n",
    "        traces = np.hstack(traces)\n",
    "\n",
    "        # Save the recording with selected channels only\n",
    "        segment_recording = se.NumpyRecording(traces_list=traces.T, sampling_frequency=sampling_frequency)\n",
    "        segment_recording.save(folder=f'{recordings_folder}/segment{segment_index}')\n",
    "    session_info = pd.json_normalize(session_info)\n",
    "    session_info.to_csv(session_info_file, index=False)\n",
    "    \n",
    "session_info = pd.read_csv(session_info_file)\n",
    "session_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and concatenate data to get \"recording\"\n",
    "Optional to change to adapt new data: freq_min, freq_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factual parameters\n",
    "n_s_per_min = 60\n",
    "n_ms_per_s = 1000\n",
    "\n",
    "# Concatenate data\n",
    "n_segment = len(session_info['segment'].unique())\n",
    "\n",
    "recordings = []\n",
    "for segment_index in range(n_segment):\n",
    "    segment_recording = sc.load_extractor(f'{recordings_folder}/segment{segment_index}')\n",
    "    \n",
    "    \"\"\"\n",
    "    # Adjust frequency (freq_min and freq_max) of filtering as needed\n",
    "    \"\"\"\n",
    "    segment_recording = spre.bandpass_filter(segment_recording, freq_min=300, freq_max=3000)\n",
    "    \n",
    "    segment_recording = spre.common_reference(segment_recording, reference='global', operator='median')\n",
    "    segment_recording = segment_recording.set_probe(probe)\n",
    "    recordings.append(segment_recording)\n",
    "recording = sc.concatenate_recordings(recordings)\n",
    "recording = recording.set_probe(probe)\n",
    "n_frames_per_ms = recording.sampling_frequency // n_ms_per_s\n",
    "recording\n",
    "\n",
    "# Get basic information of concatenated data\n",
    "n_frames_per_ms = recording.sampling_frequency // n_ms_per_s\n",
    "channel_ids = recording.get_channel_ids()\n",
    "channel_num = len(channel_ids)\n",
    "sampling_frequency = recording.get_sampling_frequency()\n",
    "recording_time = recording.get_total_duration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort data to get \"sorting\"\n",
    "Optional to change to adapt new data:other parameters for sorting as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Adjust other parameters for sorting as needed\n",
    "\"\"\"\n",
    "sorter_parameters = {\n",
    "    'detect_sign': -1,\n",
    "    'adjacency_radius': -1, \n",
    "    'freq_min': None, \n",
    "    'freq_max': None,\n",
    "    'filter': False,\n",
    "    'whiten': True,  \n",
    "    'clip_size': 50,\n",
    "    'num_workers': 8,\n",
    "    'detect_interval': 9,\n",
    "    'detect_threshold': sorter_threshold,\n",
    "}\n",
    "\n",
    "if not os.path.isfile(f'{sortings_folder}/sorter_output/firings.npz'):\n",
    "    ss.run_sorter(\n",
    "        sorter_name='mountainsort4',\n",
    "        recording=recording,\n",
    "        folder = sortings_folder,\n",
    "        remove_existing_folder=True,\n",
    "        with_output=False,\n",
    "        **sorter_parameters,\n",
    "    )\n",
    "\n",
    "sorting = se.NpzSortingExtractor(f'{sortings_folder}/sorter_output/firings.npz')\n",
    "sorting = scu.remove_excess_spikes(sorting, recording)\n",
    "splitted_sorting = sc.split_sorting(sorting, recordings)\n",
    "sortings = [sc.select_segment_sorting(splitted_sorting, segment_indices=segment) for segment in range(n_segment)]\n",
    "\n",
    "print(f'Number of units: {len(sorting.unit_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate \"analyzer\" for all segments and \"analyzers\" for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the \"analyzer\" that is used with new version of Spikeinterface\n",
    "\n",
    "# Generate \"analyzer\"\n",
    "if os.path.exists(analyzer_folder):\n",
    "    # If the folder exists, load the existing analyzer\n",
    "    analyzer = si.load_sorting_analyzer(analyzer_folder)\n",
    "    print(\"Analyzer loaded from existing folder.\")\n",
    "else:\n",
    "    # If the folder doesn't exist, create a new analyzer\n",
    "    analyzer = si.create_sorting_analyzer(sorting=sorting,\n",
    "                                    recording=recording,\n",
    "                                    format=\"binary_folder\",\n",
    "                                    return_scaled=True,  # Default is to return scaled\n",
    "                                    folder=analyzer_folder\n",
    "                                    )\n",
    "    # Compute necessary extensions for following processing-----------------------\n",
    "    analyzer.compute(\"random_spikes\", method=\"all\")\n",
    "    analyzer.compute(\"waveforms\", ms_before=1.5,ms_after=2.0)\n",
    "    analyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\n",
    "    analyzer.compute(\"noise_levels\")\n",
    "    analyzer.compute(\"principal_components\", n_components=3, mode=\"by_channel_global\", whiten=True)\n",
    "    analyzer.compute(input=\"template_similarity\", method='cosine_similarity')\n",
    "    analyzer.compute(\"spike_amplitudes\", peak_sign=\"neg\")\n",
    "    analyzer.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "    analyzer.compute(\"template_metrics\", include_multi_channel_metrics=True)\n",
    "    analyzer.compute(\"correlograms\",window_ms=50.0,bin_ms=1.0,method=\"auto\")\n",
    "    analyzer.compute(\"isi_histograms\",window_ms=50.0,bin_ms=1.0,method=\"auto\")\n",
    "    #------------------------------------------------------------------------------ \n",
    "    print(\"New analyzer created and saved.\")\n",
    "print(analyzer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate \"quality metrics\" based on \"analyzer\" and \"analyzers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for saving the quality metrics\n",
    "quality_metrics_path = f\"{output_root}/quality_metrics-{sorter_threshold}.csv\"\n",
    "\n",
    "# Check if the quality metrics CSV already exists\n",
    "if os.path.exists(quality_metrics_path):\n",
    "    print(f\"Metrics file already exists at {quality_metrics_path}. Skipping calculations.\")\n",
    "    quality_metrics = pd.read_csv(quality_metrics_path, index_col=0)\n",
    "else:\n",
    "    # List of metrics to compute\n",
    "    metrics_to_compute = [\n",
    "        'firing_rate', \n",
    "        'amplitude_median',\n",
    "        'isi_violation', \n",
    "        'silhouette',  \n",
    "        'snr',\n",
    "    ]\n",
    "\n",
    "    # Compute quality metrics\n",
    "    quality_metrics = sqm.compute_quality_metrics(\n",
    "        analyzer,\n",
    "        metric_names=metrics_to_compute\n",
    "    )\n",
    "\n",
    "    # Save the metrics to a CSV file\n",
    "    quality_metrics.to_csv(quality_metrics_path)\n",
    "    print(f\"Metrics saved to {quality_metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metrics to compute\n",
    "metrics_to_compute = [\n",
    "    'firing_rate', \n",
    "    'amplitude_median',\n",
    "    'isi_violation', \n",
    "    'silhouette',  \n",
    "    'snr',\n",
    "]\n",
    "\n",
    "# Check and process each segment independently\n",
    "for segment_idx, (recording, sorting) in enumerate(zip(recordings, sortings)):\n",
    "    # Define the folder and file path for the current segment's metrics\n",
    "    segment_analyzer_folder = os.path.join(analyzers_folder, f\"segment_{segment_idx}\")\n",
    "    segment_metrics_path = f\"{segment_analyzer_folder}/quality_metrics_segment_{segment_idx}.csv\"\n",
    "    \n",
    "    if os.path.exists(segment_metrics_path):\n",
    "        print(f\"Metrics for segment {segment_idx} already exist. Skipping calculations.\")\n",
    "        continue\n",
    "    \n",
    "    # Load the analyzer for the current segment\n",
    "    if os.path.exists(segment_analyzer_folder):\n",
    "        analyzer = si.load_sorting_analyzer(segment_analyzer_folder)\n",
    "        print(f\"Analyzer for segment {segment_idx} loaded.\")\n",
    "    else:\n",
    "        print(f\"Analyzer for segment {segment_idx} does not exist. Skipping this segment.\")\n",
    "        continue\n",
    "    \n",
    "    # Compute quality metrics for this segment\n",
    "    segment_metrics = sqm.compute_quality_metrics(\n",
    "        analyzer,\n",
    "        metric_names=metrics_to_compute\n",
    "    )\n",
    "    \n",
    "    # Save segment-specific metrics to a CSV file\n",
    "    segment_metrics.to_csv(segment_metrics_path)\n",
    "    print(f\"Metrics for segment {segment_idx} saved to {segment_metrics_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define basic plot functions for curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot functions based on unit without saving functions\n",
    "def unit_based_plot(sorting, analyzer, selected_units):\n",
    "    \"\"\"\n",
    "    Plot template, unit probe map, ISI distribution,  autocorrelogram for each unit in selected_units.\n",
    "\n",
    "    Parameters:\n",
    "    - sorting: sorting of Spikeinterface\n",
    "    - analyzer: analyzer of Spikeinterface\n",
    "    - waveforms: waveforms of Spikeinterface\n",
    "    - selected_units: A list of unit IDs to plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through the selected units\n",
    "    for unit_id in selected_units:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "        fig.suptitle(f'Plots for Unit {unit_id}', fontsize=16)\n",
    "                \n",
    "        # Plot template for the current unit \n",
    "        sw.plot_unit_templates(\n",
    "            analyzer,\n",
    "            unit_ids=[unit_id],\n",
    "            ax=axs[0],\n",
    "            same_axis=True,\n",
    "            plot_channels=\"all\",\n",
    "            sparsity=None  # This ensures all channels are plotted\n",
    "        )\n",
    "        axs[0].set_title('Template (Whole Probe)')\n",
    "                            \n",
    "        # Plot template for the current unit \n",
    "        sw.plot_unit_templates(\n",
    "            analyzer,\n",
    "            unit_ids=[unit_id],\n",
    "            ax=axs[1],\n",
    "            same_axis=True,\n",
    "            sparsity=None  # This ensures all channels are plotted\n",
    "        )\n",
    "        axs[1].set_title('Template (Local)')\n",
    "                             \n",
    "        # Plot ISI distribution for the current unit \n",
    "        sw.plot_isi_distribution(sorting.select_units([unit_id]), ax=axs[2])\n",
    "        axs[2].set_title('ISI Distribution')\n",
    "         \n",
    "        # Plot autocorrelogram for the current unit \n",
    "        sw.plot_autocorrelograms(analyzer, unit_ids=[unit_id], ax=axs[3])\n",
    "        axs[3].set_title('Autocorrelogram')\n",
    "        \n",
    "        # Adjust layout for better appearance\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "# Define plot functions based on unit with saving function\n",
    "def unit_based_plot_save(sorting, analyzer, selected_units):\n",
    "    \"\"\"\n",
    "    Plot template, unit probe map, ISI distribution,  autocorrelogram for each unit in selected_units.\n",
    "\n",
    "    Parameters:\n",
    "    - sorting: sorting of Spikeinterface\n",
    "    - analyzer: analyzer of Spikeinterface\n",
    "    - waveforms: waveforms of Spikeinterface\n",
    "    - selected_units: A list of unit IDs to plot.\n",
    "    \"\"\"\n",
    "    # Loop through the selected units\n",
    "    for unit_id in selected_units:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "        fig.suptitle(f'Plots for Unit {unit_id}', fontsize=16)\n",
    "                \n",
    "        # Plot template for the current unit \n",
    "        sw.plot_unit_templates(\n",
    "            analyzer,\n",
    "            unit_ids=[unit_id],\n",
    "            ax=axs[0],\n",
    "            same_axis=True,\n",
    "            plot_channels=\"all\",\n",
    "            sparsity=None  # This ensures all channels are plotted\n",
    "        )\n",
    "        axs[0].set_title('Template (Whole Probe)')\n",
    "                        \n",
    "        # Plot template for the current unit \n",
    "        sw.plot_unit_templates(\n",
    "            analyzer,\n",
    "            unit_ids=[unit_id],\n",
    "            ax=axs[1],\n",
    "            same_axis=True,\n",
    "            sparsity=None  # This ensures all channels are plotted\n",
    "        )\n",
    "        axs[1].set_title('Template (Local)')\n",
    "                        \n",
    "        # Plot ISI distribution for the current unit \n",
    "        sw.plot_isi_distribution(sorting.select_units([unit_id]), ax=axs[2])\n",
    "        axs[2].set_title('ISI Distribution')\n",
    "        \n",
    "        # Plot autocorrelogram for the current unit \n",
    "        sw.plot_autocorrelograms(analyzer, unit_ids=[unit_id], ax=axs[3])\n",
    "        axs[3].set_title('Autocorrelogram')\n",
    "        \n",
    "        # Adjust layout for better appearance\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        fig.savefig(os.path.join(output_root, f\"unit_{unit_id}_properties.svg\"), format=\"svg\")\n",
    "        plt.show()       \n",
    "        \n",
    "# Define plot functions based on probe\n",
    "def probe_based_plot(sorting, analyzer, selected_units):\n",
    "    \"\"\"\n",
    "    Plot a raster plot, unit presence, unit locations, and the unit-probe map for the selected units based on the probe configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    - sorting: The sorting object containing spike times and unit information.\n",
    "    - analyzer: The analyzer object containing computed waveform templates and other analysis data.\n",
    "    - selected_units: A list of unit IDs to include in the plots.\n",
    "    \"\"\"\n",
    "    # Select the units you want to plot\n",
    "    sorting_selected = sorting.select_units(selected_units)\n",
    "    analyzer_selected = analyzer.select_units(selected_units)\n",
    "        \n",
    "    # Plot the raster plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sw.plot_rasters(sorting_selected, time_range=(3610,4210),ax=ax)\n",
    "    plt.title(f'Raster Plot for Units: {selected_units}')\n",
    "    fig.savefig(os.path.join(output_root, \"raster_plot.svg\"), format=\"svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the unit presence over time\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sw.plot_unit_presence(sorting_selected, ax=ax)\n",
    "    plt.title(f'Unit Presence Plot for Units: {selected_units}')\n",
    "    fig.savefig(os.path.join(output_root, \"unit_presence.svg\"), format=\"svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the unit locations\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sw.plot_unit_locations(analyzer_selected, plot_legend=True, ax=ax)\n",
    "    plt.title(f'Unit Locations for Units: {selected_units}')\n",
    "    fig.savefig(os.path.join(output_root, \"unit_locations.svg\"), format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# First round curation based on unit template and ISI\n",
    "\"\"\"\n",
    "first_round_curated_units=[]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Second round curation based on \"quality metrics\"\n",
    "\"\"\"\n",
    "first_round_metrics = quality_metrics.loc[first_round_curated_units]\n",
    "\n",
    "# Set the curation parameter based on the quality metrics------------\n",
    "isi_violations_ratio_threshold = 2 # quantify the proportion of spikes that violate the refractory period, range >0, <0.5 is great, >2 is bad\n",
    "snr_threshold = 3 # >5 is great, 3-5 is ok, <3 is bad\n",
    "silhouette_threshold = 0.1  # silhouette_score quantify separation from other units, range from -1 to 1. Good Quality: Silhouette Score > 0.5, Moderate Quality: Silhouette Score 0.1 to 0.5\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Apply the keep_mask only to units from the first round of curation\n",
    "#keep_mask = (first_round_metrics[\"isi_violations_ratio\"] < isi_violations_ratio_threshold) & (first_round_metrics[\"snr\"] > snr_threshold)\n",
    "keep_mask = (first_round_metrics[\"isi_violations_ratio\"] < isi_violations_ratio_threshold) & (first_round_metrics[\"snr\"] > snr_threshold) & (first_round_metrics[\"silhouette\"] > silhouette_threshold)\n",
    "final_curated_units = first_round_metrics[keep_mask].index.values\n",
    "final_curated_units = [unit_id for unit_id in final_curated_units]\n",
    "print(f\"Final curated units based on quality metrics: {final_curated_units}\")\n",
    "\n",
    "# Define the final curated data with native Python integers\n",
    "final_curated_data = [int(unit_id) for unit_id in final_curated_units]\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(output_root, \"unit_locations.svg\"), format=\"svg\")\n",
    "\n",
    "# Save curated data in JSON format in the same output path\n",
    "curated_data_path = os.path.join(output_root, \"final_curated_data.json\")\n",
    "with open(curated_data_path, \"w\") as f:\n",
    "    json.dump(final_curated_data, f)\n",
    "print(f\"Curated data saved to {curated_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot firing change base on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store firing rates for each unit across segments\n",
    "curated_units_firing_rate = {}\n",
    "\n",
    "# Iterate over all segments and load the metrics\n",
    "for segment_idx in range(len(recordings)):\n",
    "    # Define the metrics file path for the current segment\n",
    "    segment_metrics_path = os.path.join(analyzers_folder, f\"segment_{segment_idx}\", f\"quality_metrics_segment_{segment_idx}.csv\")\n",
    "    \n",
    "    if not os.path.exists(segment_metrics_path):\n",
    "        print(f\"Metrics file for segment {segment_idx} not found. Skipping this segment.\")\n",
    "        continue\n",
    "    \n",
    "    # Load the quality metrics for the current segment\n",
    "    segment_metrics = pd.read_csv(segment_metrics_path, index_col=0)\n",
    "    \n",
    "    # Iterate through all curated units and store their firing_rate\n",
    "    for unit_id in final_curated_units:\n",
    "        if unit_id in segment_metrics.index:\n",
    "            if segment_idx not in curated_units_firing_rate:\n",
    "                curated_units_firing_rate[segment_idx] = []\n",
    "            firing_rate = segment_metrics.loc[unit_id, \"firing_rate\"]\n",
    "            if not pd.isna(firing_rate):  # Exclude NaN firing rates\n",
    "                curated_units_firing_rate[segment_idx].append(firing_rate)\n",
    "        else:\n",
    "            print(f\"Unit {unit_id} not found in segment {segment_idx}. Skipping.\")\n",
    "\n",
    "# Prepare data for bar plot and calculate t-test p-values\n",
    "segment_indices = []\n",
    "average_firing_rates = []\n",
    "firing_rate_sds = []\n",
    "p_values = []\n",
    "\n",
    "for segment_idx, firing_rates in curated_units_firing_rate.items():\n",
    "    segment_indices.append(segment_idx)\n",
    "    average_firing_rates.append(np.mean(firing_rates))\n",
    "    firing_rate_sds.append(np.std(firing_rates))\n",
    "\n",
    "# Calculate p-values for consecutive segments\n",
    "for i in range(len(segment_indices) - 1):\n",
    "    segment_1_rates = curated_units_firing_rate[segment_indices[i]]\n",
    "    segment_2_rates = curated_units_firing_rate[segment_indices[i + 1]]\n",
    "    \n",
    "    # Perform un-paired t-test\n",
    "    t_stat, p_value = ttest_ind(segment_1_rates, segment_2_rates, equal_var=False)\n",
    "    p_values.append((segment_indices[i], segment_indices[i + 1], p_value))\n",
    "\n",
    "# Plot the bar plot with gray dots for each unit's firing rate\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_positions = np.arange(len(segment_indices))\n",
    "plt.bar(bar_positions, average_firing_rates, yerr=firing_rate_sds, capsize=5, color='lightblue', alpha=0.7, label='Average Firing Rate')\n",
    "\n",
    "# Plot gray dots on the bars for individual unit firing rates\n",
    "for i, (segment_idx, firing_rates) in enumerate(curated_units_firing_rate.items()):\n",
    "    plt.scatter([bar_positions[i]] * len(firing_rates), firing_rates, color='gray', alpha=0.6, label='_nolegend_')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(bar_positions, segment_indices)\n",
    "plt.title(\"Average Firing Rate Across Segments\")\n",
    "plt.xlabel(\"Segment Index\")\n",
    "plt.ylabel(\"Firing Rate (Hz)\")\n",
    "plt.legend(title=\"Data\", loc='upper right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "figure_path = os.path.join(output_root, \"average_firing_rate_across_segments.svg\")\n",
    "plt.savefig(figure_path, format='svg')\n",
    "print(f\"Figure saved to {figure_path}\")\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n",
    "\n",
    "# Display p-values\n",
    "print(\"Un-paired t-test p-values between consecutive segments:\")\n",
    "for seg_1, seg_2, p_val in p_values:\n",
    "    print(f\"Segment {seg_1} vs. Segment {seg_2}: p-value = {p_val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate file for DataHigh neural population analysis in Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datahigh_input(\n",
    "    curated_units,\n",
    "    recording_duration,                # The total time of concatenated segments (in second)\n",
    "    segment_duration,                   # The time of each segment (in second, up to 10 conditions or segments)\n",
    "    time_range,                  # The time range in each segment that will be processed (in second)\n",
    "    trial_time,                          # The trail time in each time range (in second)\n",
    "    bin_size                           # The bin_size in each trail (in second, should be adjusted to allow the data array has reasonable number of '1')\n",
    "):\n",
    "    # Define output file name\n",
    "    output_file = os.path.join(output_root, \"datahigh_input.mat\")\n",
    "    \n",
    "    # Calculate the number of conditions\n",
    "    num_conditions = int(recording_duration / segment_duration)\n",
    "    conditions = [f\"condition_{i+1}\" for i in range(num_conditions)]\n",
    "    \n",
    "    # Define colors for up to 10 conditions\n",
    "    base_colors = [\n",
    "        [1, 0, 0],    # Red\n",
    "        [0, 1, 0],    # Green\n",
    "        [0, 0, 1],    # Blue\n",
    "        [1, 1, 0],    # Yellow\n",
    "        [1, 0, 1],    # Magenta\n",
    "        [0, 1, 1],    # Cyan\n",
    "        [0.5, 0.5, 0.5],  # Gray\n",
    "        [0.5, 0, 0],  # Dark Red\n",
    "        [0, 0.5, 0],  # Dark Green\n",
    "        [0, 0, 0.5]   # Dark Blue\n",
    "    ]\n",
    "    epoch_colors = base_colors[:num_conditions]  # Use only as many colors as conditions\n",
    "    \n",
    "    # Calculate the number of trials within the time_range for each condition\n",
    "    num_trials_per_condition = int((time_range[1] - time_range[0]) / trial_time)\n",
    "    total_trials = num_trials_per_condition * num_conditions\n",
    "    \n",
    "    # Initialize the structured array with the specified field order\n",
    "    D = np.empty(total_trials, dtype=[('data', 'O'), ('epochStarts', 'O'), ('epochColors', 'O'), ('condition', 'O')])\n",
    "    \n",
    "    # Initialize dictionary to store 1 ratios for each condition\n",
    "    condition_ratios = {}\n",
    "    \n",
    "    # Loop over each condition\n",
    "    trial_counter = 0\n",
    "    for condition_idx, condition_name in enumerate(conditions):\n",
    "        # Define the start time of the condition in the overall recording\n",
    "        condition_start_time = condition_idx * segment_duration\n",
    "        color = epoch_colors[condition_idx]\n",
    "        \n",
    "        # Define the start and end times for this time_range within the condition\n",
    "        range_start = condition_start_time + time_range[0]\n",
    "        range_end = condition_start_time + time_range[1]\n",
    "        \n",
    "        # Initialize counters for this condition\n",
    "        condition_ones = 0\n",
    "        condition_bins = 0\n",
    "        \n",
    "        # Generate trials within the specified time range for this condition\n",
    "        for trial_num in range(num_trials_per_condition):\n",
    "            trial_start_time = range_start + trial_num * trial_time\n",
    "            trial_end_time = trial_start_time + trial_time\n",
    "            \n",
    "            # Initialize an empty list to hold data for each unit\n",
    "            trial_data = []\n",
    "\n",
    "            for unit_id in curated_units:\n",
    "                # Get spike times for the unit and filter by trial window within the specified time range\n",
    "                unit_spike_s = sorting.get_unit_spike_train(unit_id) / sorting.get_sampling_frequency()\n",
    "                trial_spikes = unit_spike_s[(unit_spike_s >= trial_start_time) & (unit_spike_s < trial_end_time)]\n",
    "                \n",
    "                # Bin the spike times into 1 ms bins\n",
    "                bins = np.arange(trial_start_time, trial_end_time, bin_size)\n",
    "                binned_spikes, _ = np.histogram(trial_spikes, bins=bins)\n",
    "                \n",
    "                # Convert to binary (0's and 1's)\n",
    "                binary_spikes = (binned_spikes > 0).astype(int)\n",
    "                \n",
    "                # Count 1s and total bins for ratio calculation for this condition\n",
    "                condition_ones += np.sum(binary_spikes)\n",
    "                condition_bins += binary_spikes.size\n",
    "                \n",
    "                # Append to trial data\n",
    "                trial_data.append(binary_spikes)\n",
    "\n",
    "            # Convert trial_data to a matrix (neurons x time bins)\n",
    "            trial_matrix = np.array(trial_data)\n",
    "            \n",
    "            # Assign values to the structured array fields\n",
    "            D[trial_counter]['data'] = trial_matrix  # Set data to the spike train matrix\n",
    "            D[trial_counter]['epochStarts'] = np.array([1])  # Set epochStarts to 1 for all trials\n",
    "            D[trial_counter]['epochColors'] = np.array([color])  # Set color based on condition\n",
    "            D[trial_counter]['condition'] = condition_name  # Assign the condition name\n",
    "            \n",
    "            # Increment the trial counter\n",
    "            trial_counter += 1\n",
    "\n",
    "        # Calculate the 1 ratio for this condition\n",
    "        condition_ratio = condition_ones / condition_bins if condition_bins > 0 else 0\n",
    "        condition_ratios[condition_name] = condition_ratio\n",
    "        print(f\"Condition {condition_name} - Ratio of 1s: {condition_ratio:.4f} ({condition_ratio * 100:.2f}%)\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    \n",
    "    # Save as a .mat file\n",
    "    savemat(output_file, {\"D\": D})\n",
    "    \n",
    "    # Return the condition ratios dictionary for further analysis if needed\n",
    "    return condition_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_datahigh_input(\n",
    "    curated_units=final_curated_units,\n",
    "    recording_duration=4200,                # The total time of concatenated segments (in second)\n",
    "    segment_duration=600,                   # The time of each segment (in second, up to 10 conditions or segments)\n",
    "    time_range=(300,500),                  # The time range in each segment that will be processed (in second)\n",
    "    trial_time=10,                          # The trail time in each time range (in second)\n",
    "    bin_size=0.001                           # The bin_size in each trail (in second, should be adjusted to allow the data array has reasonable number of '1')\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "A 1-10% ratio of 1s in the data is generally effective for GPFA, with adjustments to bin size if needed to achieve this range. \n",
    "This provides a balance between capturing meaningful structure and avoiding an excess of zeros or too much noise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 2D UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_with_filtering(analyzer, curated_unit_ids, output_root, nb_points=600):\n",
    "    \"\"\"\n",
    "    Generate and plot a UMAP projection for curated sorted units with filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    - analyzer: The SortingAnalyzer object with precomputed principal components.\n",
    "    - curated_unit_ids: A list of unit IDs to include in the UMAP plot.\n",
    "    - output_root: Path to save the UMAP plot.\n",
    "    - nb_points: Number of closest PCA points to select for each unit.\n",
    "    \"\"\"\n",
    "    # Retrieve the PCA extension\n",
    "    ext_pca = analyzer.get_extension(\"principal_components\")\n",
    "\n",
    "    # Collect filtered PCA scores for only the curated units\n",
    "    all_pca_data = []\n",
    "    all_labels = []\n",
    "    for unit_id in curated_unit_ids:\n",
    "        # Retrieve PCA projections for the current unit\n",
    "        unit_pca = ext_pca.get_projections_one_unit(unit_id=unit_id, sparse=False)\n",
    "        \n",
    "        # Skip units with no PCA projections\n",
    "        if unit_pca.size == 0:\n",
    "            print(f\"Skipping unit {unit_id} due to empty PCA projections.\")\n",
    "            continue\n",
    "        \n",
    "        # Flatten across components and channels: (num_spikes, num_pca_components * num_channels)\n",
    "        unit_pca_flattened = unit_pca.reshape(unit_pca.shape[0], -1)\n",
    "\n",
    "        # Compute distances to the mean PCA projection\n",
    "        mean_pca = np.mean(unit_pca_flattened, axis=0)\n",
    "        distances = np.sqrt(np.sum(np.square(unit_pca_flattened - mean_pca), axis=1))\n",
    "\n",
    "        # Select the closest `nb_points` projections\n",
    "        if len(distances) > nb_points:\n",
    "            selected_indices = np.argsort(distances)[:nb_points]\n",
    "            unit_pca_filtered = unit_pca_flattened[selected_indices]\n",
    "        else:\n",
    "            unit_pca_filtered = unit_pca_flattened\n",
    "\n",
    "        # Append the filtered PCA projections and corresponding labels\n",
    "        all_pca_data.append(unit_pca_filtered)\n",
    "        all_labels.extend([unit_id] * len(unit_pca_filtered))\n",
    "\n",
    "    # Check if there is any data to process\n",
    "    if not all_pca_data:\n",
    "        print(\"No valid PCA data found for the curated units. UMAP plot cannot be generated.\")\n",
    "        return\n",
    "\n",
    "    # Concatenate all PCA data into a single 2D array for UMAP\n",
    "    all_pca_data = np.vstack(all_pca_data)\n",
    "\n",
    "    # Apply UMAP with enhanced separation settings\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=30,       # Increase for more global structure\n",
    "        min_dist=0.1,         # Decrease for tighter clusters\n",
    "        metric=\"cosine\",      # Use cosine similarity for better high-dimensional structure\n",
    "        random_state=42\n",
    "    )\n",
    "    embedding = reducer.fit_transform(all_pca_data)\n",
    "\n",
    "    # Assign distinct colors for each unit using Method 2\n",
    "    unique_units = np.unique(curated_unit_ids)\n",
    "    num_colors = len(unique_units)\n",
    "    cmap = plt.cm.get_cmap('tab20', max(num_colors, 40))  # Ensure at least 40 colors available\n",
    "    unit_color_dict = {unit: cmap(i / num_colors) for i, unit in enumerate(unique_units)}\n",
    "\n",
    "    # Generate colors for the embedding\n",
    "    colors = [unit_color_dict[label] for label in all_labels]\n",
    "\n",
    "    # Plot the UMAP result\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=colors, s=5, alpha=0.8)\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.title(\"UMAP of Curated Units (Filtered PCA) for Segment 5\")\n",
    "\n",
    "    # Add a legend for unit IDs\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=unit_color_dict[unit], markersize=10) \n",
    "               for unit in unique_units]\n",
    "    plt.legend(handles, [f\"Unit {unit}\" for unit in unique_units], title=\"Unit IDs\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_root, \"UMAP_segment_5_filtered.svg\")\n",
    "    plt.savefig(output_path, format=\"svg\", bbox_inches='tight')\n",
    "    print(f\"UMAP plot saved to {output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with segment 5 analyzer\n",
    "plot_umap_with_filtering(analyzer_segment_5, final_curated_units, output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results of final curated units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit_based_plot_save(sorting, analyzer, final_curated_units)\n",
    "probe_based_plot(sorting, analyzer, final_curated_units)\n",
    "#plot_umap(analyzer, final_curated_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot pearson correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_matrix(curated_units=final_curated_units, sorting=sorting, start_time=1200, end_time=1800):\n",
    "    \"\"\"\n",
    "    Compute a correlation matrix for a list of curated units based on spike train overlap.\n",
    "\n",
    "    Parameters:\n",
    "        curated_units (list of int): List of curated unit IDs.\n",
    "        sorting (SortingExtractor): The sorting extractor with the spike data.\n",
    "        start_time (float): Start time of the interval for filtering spikes.\n",
    "        end_time (float): End time of the interval for filtering spikes.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Correlation matrix where each element represents the correlation between two units.\n",
    "    \"\"\"\n",
    "    # Extract spike trains for each curated unit within the specified time range\n",
    "    curated_spike_trains = [\n",
    "        (sorting.get_unit_spike_train(unit_id) / sorting.get_sampling_frequency())[\n",
    "            (sorting.get_unit_spike_train(unit_id) / sorting.get_sampling_frequency() >= start_time) & \n",
    "            (sorting.get_unit_spike_train(unit_id) / sorting.get_sampling_frequency() < end_time)\n",
    "        ]\n",
    "        for unit_id in curated_units\n",
    "    ]\n",
    "\n",
    "    n_units = len(curated_spike_trains)\n",
    "    correlation_matrix = np.zeros((n_units, n_units))\n",
    "\n",
    "    # Compute correlation between each pair of units\n",
    "    for i in range(n_units):\n",
    "        for j in range(n_units):\n",
    "            if i != j:  # Skip autocorrelation\n",
    "                if len(curated_spike_trains[i]) > 1 and len(curated_spike_trains[j]) > 1:\n",
    "                    # Bin the spike trains to a common resolution (e.g., 100 ms bins) for cross-correlation\n",
    "                    bin_edges = np.arange(start_time, end_time, 0.1)  # 50 ms bin size\n",
    "                    binned_i, _ = np.histogram(curated_spike_trains[i], bins=bin_edges)\n",
    "                    binned_j, _ = np.histogram(curated_spike_trains[j], bins=bin_edges)\n",
    "                    \n",
    "                    # Calculate Pearson correlation coefficient\n",
    "                    correlation = np.corrcoef(binned_i, binned_j)[0, 1]\n",
    "                    correlation_matrix[i, j] = correlation if not np.isnan(correlation) else 0\n",
    "\n",
    "    # Set diagonal values to NaN to display as white\n",
    "    np.fill_diagonal(correlation_matrix, np.nan)\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "def plot_correlation_heatmap(correlation_matrix, curated_units=final_curated_units):\n",
    "    \"\"\"\n",
    "    Plot the correlation matrix as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "        correlation_matrix (np.ndarray): Matrix of correlations between units.\n",
    "        curated_units (list of int): List of curated unit IDs for labeling.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Display heatmap with NaN values as white, and adjust color range to -0.1 to 0.1\n",
    "    cax = ax.imshow(correlation_matrix, cmap='viridis', vmin=-0.1, vmax=0.1)\n",
    "\n",
    "    # Add color bar with a range from -0.1 to 0.1\n",
    "    fig.colorbar(cax, ax=ax, label='Correlation', ticks=[-0.1, 0, 0.1])\n",
    "\n",
    "    # Label the axes with unit numbers\n",
    "    ax.set_xticks(np.arange(len(curated_units)))\n",
    "    ax.set_yticks(np.arange(len(curated_units)))\n",
    "    ax.set_xticklabels(curated_units)\n",
    "    ax.set_yticklabels(curated_units)\n",
    "    ax.set_xlabel(\"Unit\")\n",
    "    ax.set_ylabel(\"Unit\")\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(output_root, \"correlation_heatmap_1200-1800.svg\"), format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "# Compute the correlation matrix and plot the heatmap\n",
    "correlation_matrix = compute_correlation_matrix()\n",
    "plot_correlation_heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot firing rate changes along recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_firing_rate_change(recording, sorting, bin_size, curated_unit_ids, time_ranges, segment_time):\n",
    "    \"\"\"\n",
    "    Plot the change of firing rate along the recording time for curated units with the specified bin size\n",
    "    and plot the averaged firing rate across all curated units within specified time ranges.\n",
    "    Also, calculate the change in firing rate between segments and plot individual changes with averaged changes.\n",
    "\n",
    "    Parameters:\n",
    "    - recording: The recording object.\n",
    "    - sorting: The sorting object containing spike times and unit information.\n",
    "    - bin_size: The bin size in seconds for the analysis.\n",
    "    - curated_unit_ids: A list of unit IDs to include in the plot.\n",
    "    - time_ranges: A list of tuples specifying time ranges (e.g., [(0, 60), (100, 120)]).\n",
    "    - segment_time: The duration of each segment in seconds for calculating firing rate changes.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_root, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    # Assign a unique color to each unit\n",
    "    color_cycle = plt.cm.tab10.colors  # Use a color map with up to 10 colors; extend if more units\n",
    "    unit_colors = {unit_id: color_cycle[i % len(color_cycle)] for i, unit_id in enumerate(curated_unit_ids)}\n",
    "\n",
    "    # Plot 1: Firing Rate Over Time\n",
    "    concatenated_firing_rates = {unit_id: [] for unit_id in curated_unit_ids}\n",
    "    concatenated_bin_centers = []\n",
    "    current_start_time = 0\n",
    "\n",
    "    for start_time, end_time in time_ranges:\n",
    "        bins = np.arange(start_time, end_time + bin_size, bin_size)\n",
    "        bin_centers = bins[:-1] + bin_size / 2\n",
    "        adjusted_bin_centers = bin_centers + current_start_time - start_time\n",
    "\n",
    "        concatenated_bin_centers.extend(adjusted_bin_centers)\n",
    "\n",
    "        for unit_id in curated_unit_ids:\n",
    "            unit_spike_train = sorting.get_unit_spike_train(unit_id) / recording.get_sampling_frequency()\n",
    "            unit_spike_train = unit_spike_train[(unit_spike_train >= start_time) & (unit_spike_train < end_time)]\n",
    "\n",
    "            firing_rate, _ = np.histogram(unit_spike_train, bins=bins)\n",
    "            firing_rate = firing_rate / bin_size\n",
    "            concatenated_firing_rates[unit_id].extend(firing_rate)\n",
    "\n",
    "        current_start_time += end_time - start_time\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for unit_id in curated_unit_ids:\n",
    "        ax.plot(concatenated_bin_centers, concatenated_firing_rates[unit_id], label=f'Unit {unit_id}', \n",
    "                marker='o', linestyle='-', color=unit_colors[unit_id])\n",
    "\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Firing Rate (spikes per second)')\n",
    "    ax.set_title('Firing Rate Change Over Time for Curated Units (Continuous)')\n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(output_root, \"firing_rate_over_time_of_each_units.svg\"), format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Average Firing Rate Across Units\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    all_rates = np.array(list(concatenated_firing_rates.values()))\n",
    "    if all_rates.size > 0:\n",
    "        average_firing_rate = np.mean(all_rates, axis=0)\n",
    "        ax.plot(concatenated_bin_centers, average_firing_rate, marker='o', linestyle='-', color='red', label='Average Firing Rate')\n",
    "    else:\n",
    "        print(\"No valid firing rates available for averaging.\")\n",
    "\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Average Firing Rate (spikes per second)')\n",
    "    ax.set_title('Average Firing Rate Across Curated Units (Continuous)')\n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(output_root, \"average_firing_rate_across_units.svg\"), format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Firing Rate Changes Between Segments (Absolute Values with p-value Calculation for Adjacent Transitions)\n",
    "    total_duration = recording.get_total_duration()\n",
    "    num_segments = int(total_duration // segment_time)\n",
    "    segment_ranges = [(i * segment_time, (i + 1) * segment_time) for i in range(num_segments)]\n",
    "\n",
    "    firing_rate_changes = {unit_id: [] for unit_id in curated_unit_ids}\n",
    "\n",
    "    for i in range(1, len(segment_ranges)):\n",
    "        prev_start, prev_end = segment_ranges[i - 1]\n",
    "        curr_start, curr_end = segment_ranges[i]\n",
    "\n",
    "        for unit_id in curated_unit_ids:\n",
    "            prev_spikes = sorting.get_unit_spike_train(unit_id) / recording.get_sampling_frequency()\n",
    "            prev_spikes = prev_spikes[(prev_spikes >= prev_start) & (prev_spikes < prev_end)]\n",
    "\n",
    "            curr_spikes = sorting.get_unit_spike_train(unit_id) / recording.get_sampling_frequency()\n",
    "            curr_spikes = curr_spikes[(curr_spikes >= curr_start) & (curr_spikes < curr_end)]\n",
    "\n",
    "            prev_rate = len(prev_spikes) / segment_time\n",
    "            curr_rate = len(curr_spikes) / segment_time\n",
    "\n",
    "            rate_change = abs(curr_rate - prev_rate)  # Take the absolute value of the rate change\n",
    "            firing_rate_changes[unit_id].append(rate_change)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    all_changes = []\n",
    "    dot_values = {}\n",
    "    for transition_index in range(len(segment_ranges) - 1):\n",
    "        dot_values[transition_index] = []\n",
    "\n",
    "    for unit_id, changes in firing_rate_changes.items():\n",
    "        for transition_index, rate_change in enumerate(changes):\n",
    "            ax.scatter(transition_index, rate_change, color=unit_colors[unit_id])\n",
    "            dot_values[transition_index].append(rate_change)\n",
    "        all_changes.append(changes)\n",
    "\n",
    "    avg_changes = np.mean(np.array(all_changes), axis=0) if len(all_changes) > 0 else []\n",
    "    std_changes = np.std(np.array(all_changes), axis=0) if len(all_changes) > 0 else []\n",
    "\n",
    "    if len(avg_changes) > 0:\n",
    "        ax.bar(np.arange(len(avg_changes)), avg_changes, color='gray', alpha=0.5, label='Average Change')\n",
    "        ax.errorbar(\n",
    "            np.arange(len(avg_changes)), avg_changes, yerr=std_changes, fmt='o', color='black', ecolor='red', \n",
    "            elinewidth=2, capsize=5, label='SD Error'\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel('Segment Transition Index')\n",
    "    ax.set_ylabel('Absolute Firing Rate Change (spikes per second)')\n",
    "    ax.set_title('Absolute Firing Rate Change Between Segments')\n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(output_root, \"absolute_firing_rate_changes_between_segments_with_sd.svg\"), format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print individual dot values grouped by transition index\n",
    "    for transition_index, values in dot_values.items():\n",
    "        print(f\"Transition {transition_index + 1}:\")\n",
    "        print(f\"{[f'{v:.4f}' for v in values]}\")\n",
    "\n",
    "    # Calculate and print p-values for adjacent transitions\n",
    "    print(\"\\nAdjacent Transition Comparisons (p-values):\")\n",
    "    for i in range(len(dot_values) - 1):\n",
    "        stat, p_value = ttest_ind(dot_values[i], dot_values[i + 1], equal_var=False)  # Perform t-test\n",
    "        print(f\"Transition {i + 1} vs Transition {i + 2}: p = {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rate_change_bin_size=30 #The bin size in seconds for the analysis, in second.\n",
    "time_ranges = [(0,4200)] #Exclude the data that has human noise.\n",
    "segment_time = 600\n",
    "#plot_firing_rate_change(recording, sorting, bin_size=firing_rate_change_bin_size, curated_unit_ids=final_curation_unit_ids)\n",
    "plot_firing_rate_change(recording, sorting, bin_size=firing_rate_change_bin_size, curated_unit_ids=final_curated_units, time_ranges=time_ranges, segment_time=segment_time)\n",
    "\"\"\"\n",
    "    Plot the change of firing rate along the recording time for curated units with the specified bin size\n",
    "    and plot the averaged firing rate across all curated units.\n",
    "    \n",
    "    Parameters:\n",
    "    - recording: The recording object.\n",
    "    - sorting: The sorting object.\n",
    "    - bin_size: The bin size in seconds for the analysis.\n",
    "    - curated_units: A list of unit IDs to include in the plot.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    How to interpret the results:\n",
    "    - First plot shows the firing rate changes of units, each line corresponds to one unit. \n",
    "    - If lots of 0 in first plot, means the bin size choose here is too small for the following neural population analysis.\n",
    "    \n",
    "    - Second plot shows the averaged firing rate changes of all units.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haosheng-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
